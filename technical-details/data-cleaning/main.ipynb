{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Cleaning\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- After digesting the instructions, you can delete this cell, these are assignment instructions and do not need to be included in your final submission.  -->\n",
    "\n",
    "{{< include instructions.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code \n",
    "\n",
    "Provide the source code used for this section of the project here.\n",
    "\n",
    "If you're using a package for code organization, you can import it at this point. However, make sure that the **actual workflow steps**—including data processing, analysis, and other key tasks—are conducted and clearly demonstrated on this page. The goal is to show the technical flow of your project, highlighting how the code is executed to achieve your results.\n",
    "\n",
    "If relevant, link to additional documentation or external references that explain any complex components. This section should give readers a clear view of how the project is implemented from a technical perspective.\n",
    "\n",
    "Remember, this page is a technical narrative, NOT just a notebook with a collection of code cells, include in-line Prose, to describe what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first handled the survey data from **Pew Research Center's American Trends Panel Wave 111**. There was a decently large sample size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6034, 139)\n",
      "Index(['QKEY', 'INTERVIEW_START_W111', 'INTERVIEW_END_W111',\n",
      "       'DEVICE_TYPE_W111', 'LANG_W111', 'XTABLET_W111', 'SHOP18_W111',\n",
      "       'SHOP19_W111', 'METOO1_W111', 'METOOSUPOE_M1_W111',\n",
      "       ...\n",
      "       'F_PARTYLN_FINAL', 'F_PARTYSUM_FINAL', 'F_PARTYSUMIDEO_FINAL',\n",
      "       'F_INC_SDT1', 'F_REG', 'F_IDEO', 'F_INTFREQ', 'F_VOLSUM', 'F_INC_TIER2',\n",
      "       'WEIGHT_W111'],\n",
      "      dtype='object', length=139)\n"
     ]
    }
   ],
   "source": [
    "# Read in .sav file\n",
    "W111_df = pd.read_spss(\"../../data/raw-data/ATP_W111.sav\")\n",
    "#print(W111_df.head())\n",
    "\n",
    "#Disply data frame shape and column titles\n",
    "print(W111_df.shape)\n",
    "print(W111_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I start off by cleaning the whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and filter  \n",
    "\n",
    "# Remove whitespace from column names     \n",
    "W111_df.columns = W111_df.columns.str.strip()\n",
    "\n",
    "for col in W111_df.columns:\n",
    "\n",
    "    # Iterate through each column name and remove the suffix if present\n",
    "    if col.endswith(\"_W111\"): # Checks if column title ends with that title\n",
    "        new_col_name = col[:-5]  # Remove that part of the name\n",
    "        W111_df = W111_df.rename(columns={col: new_col_name})\n",
    "    if col.startswith(\"F_\"):\n",
    "        new_col_name = col[2:]  # Remove the first 2 characters\n",
    "        W111_df = W111_df.rename(columns={col: new_col_name})\n",
    "\n",
    "\n",
    "# Remove whitespace from each row in each column if column data type is string\n",
    "for col in W111_df.columns:\n",
    "    if W111_df[col].dtype == \"object\":\n",
    "        W111_df[col] = W111_df[col].str.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After referring to the survey's questionnaire document (included ...) to see what each feature (column) refers too, I selected the following to look into.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONLSHOP1_a    category\n",
      "ONLSHOP1_b    category\n",
      "ONLSHOP1_c    category\n",
      "SHOP4         category\n",
      "SNSUSE        category\n",
      "ONLSHOP5      category\n",
      "MARITAL       category\n",
      "USR_SELFID    category\n",
      "AGECAT        category\n",
      "GENDER        category\n",
      "EDUCCAT       category\n",
      "RACECMB       category\n",
      "INC_SDT1      category\n",
      "dtype: object\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "W111_columns_keep = [\"ONLSHOP1_a\", \"ONLSHOP1_b\", \"ONLSHOP1_c\", \"SHOP4\", \"SNSUSE\", \"ONLSHOP5\", \"MARITAL\", \"USR_SELFID\", \"AGECAT\", \n",
    "                     \"GENDER\", \"EDUCCAT\", \"RACECMB\", \"INC_SDT1\"]\n",
    "W111_df = W111_df[W111_columns_keep]\n",
    "\n",
    "# View column data types\n",
    "print(print(W111_df.dtypes) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W111_df.rename(columns={'age': 'age_years'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONLSHOP1_a     142\n",
      "ONLSHOP1_b     142\n",
      "ONLSHOP1_c     142\n",
      "SHOP4          142\n",
      "SNSUSE         142\n",
      "ONLSHOP5      1406\n",
      "MARITAL          0\n",
      "USR_SELFID       0\n",
      "AGECAT           0\n",
      "GENDER           0\n",
      "EDUCCAT          0\n",
      "RACECMB          2\n",
      "INC_SDT1         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values per column\n",
    "null_counts = W111_df.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ONLSHOP1_a', 'ONLSHOP1_b', 'ONLSHOP1_c', 'SHOP4', 'SNSUSE', 'ONLSHOP5',\n",
      "       'MARITAL', 'USR_SELFID', 'AGECAT', 'GENDER', 'EDUCCAT', 'RACECMB',\n",
      "       'INC_SDT1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(W111_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I moved on to handling the data from the Consumer Expenditure Survey. We begin with the income data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import data for income\n",
    "income_1_df = pd.read_csv(\"../../data/raw-data/itii232.csv\")\n",
    "income_2_df = pd.read_csv(\"../../data/raw-data/itii233.csv\")\n",
    "income_3_df = pd.read_csv(\"../../data/raw-data/itii234.csv\")\n",
    "income_4_df = pd.read_csv(\"../../data/raw-data/itii241.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.describe of           NEWID  REFMO  REFYR     UCC  PUBFLAG VALUE_  IMPNUM        VALUE\n",
      "0       5090604      1   2023  900030        2    NaN       1  3169.833300\n",
      "1       5090604      1   2023  900030        2    NaN       2  3169.833300\n",
      "2       5090604      1   2023  900030        2    NaN       3  3169.833300\n",
      "3       5090604      1   2023  900030        2    NaN       4  3169.833300\n",
      "4       5090604      1   2023  900030        2    NaN       5  3169.833300\n",
      "...         ...    ...    ...     ...      ...    ...     ...          ...\n",
      "330445  5366911      5   2023  980071        2    NaN       1   820.250000\n",
      "330446  5366911      5   2023  980071        2    NaN       2   250.000000\n",
      "330447  5366911      5   2023  980071        2    NaN       3   100.000000\n",
      "330448  5366911      5   2023  980071        2    NaN       4   294.666667\n",
      "330449  5366911      5   2023  980071        2    NaN       5   160.250000\n",
      "\n",
      "[330450 rows x 8 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(income_1_df.describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I filter for the relevant columns in the income Data Frames. From the data collection stage, we already know that each of data frames has 8 columns. The variable \"NEWID\" represent the unique identifier for the survey participant. The values under variable \"UCC\" indicate certain increases or decreases to the individuals' net worth. The variable \"VALUE\" indicate the absolute value of the change in net worth. The other 5 variables only represent data reelvant to the survey process so we subset the Data Frames for those 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330450, 3)\n",
      "(330840, 3)\n",
      "(322320, 3)\n",
      "(325200, 3)\n"
     ]
    }
   ],
   "source": [
    "income_columns_keep = ['NEWID', 'UCC', 'VALUE']\n",
    "\n",
    "income_1_df = income_1_df[income_columns_keep]\n",
    "print(income_1_df.shape)\n",
    "\n",
    "income_2_df = income_2_df[income_columns_keep]\n",
    "print(income_2_df.shape)\n",
    "\n",
    "income_3_df = income_3_df[income_columns_keep]\n",
    "print(income_3_df.shape)\n",
    "\n",
    "income_4_df = income_4_df[income_columns_keep]\n",
    "print(income_4_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to find the unqiue \"UCC\" values to see if we have to deal with decreases in net worth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[900030, 900170, 900180, 980000, 980071, 800940, 900000, 900160, 900150, 900090, 900190, 900200, 900210, 900120, 900140]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize list that stores all unique values of 'UCC' column\n",
    "all_UCC_unique = []\n",
    "\n",
    "# Function that prints the unique values in a particular column and returns the list\n",
    "def find_unique_UCC_values(df, column_name):\n",
    "\n",
    "  unique_values = df[column_name].unique()\n",
    "  for value in unique_values:\n",
    "    if value not in all_UCC_unique:\n",
    "        all_UCC_unique.append(value)\n",
    "        \n",
    "  \n",
    "find_unique_UCC_values(income_1_df, 'UCC')\n",
    "find_unique_UCC_values(income_2_df, 'UCC')\n",
    "find_unique_UCC_values(income_3_df, 'UCC')\n",
    "find_unique_UCC_values(income_4_df, 'UCC')\n",
    "\n",
    "print(all_UCC_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By referring to the data dictionary, I found that the \"UCC\" values are mostly associated with increases, except for 800940 which represents deductions for social security. \n",
    "There is some overlap between them. For example, 980071 represent income after taxes. Here I want to only focus on pre-tax income for simplicity's sake. Therefore we filter for the following:\n",
    "- 900030: Social Security and railroad retirement income\n",
    "- 900170: Retirement, survivors, disability income\n",
    "- 900180: Interest and dividends\n",
    "- 980000: Income before taxes\n",
    "- 800940: Deductions for Social Security\n",
    "- 900150: Food stamps\n",
    "\n",
    "The following codes correspond to income that is lumped into 980000: Income before taxes\n",
    "- 900160: Self-employment income\n",
    "- 900000: Wages and salaries \n",
    "- 900090: Supplemental security income\n",
    "- 900190: Net room/rental income\n",
    "- 900200: Royalty, estate, trust income\n",
    "- 900210: Other regular income\n",
    "- 900140: Other income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182790, 3)\n",
      "(182475, 3)\n",
      "(178470, 3)\n",
      "(179925, 3)\n"
     ]
    }
   ],
   "source": [
    "income_df_UCC_keep = [900030, 900170, 900180, 980000, 800940, 900150]\n",
    "\n",
    "negation_UCC_value = 800940\n",
    "\n",
    "# Function to filter for the 'UCC' values we want and negate if UCC = 800940\n",
    "def filter_and_negate(df, negation_ucc):\n",
    "\n",
    "  # Filter the DataFrame based on the UCC list\n",
    "  filtered_df = df[df['UCC'].isin(income_df_UCC_keep)]\n",
    "\n",
    "  # Negate the 'VALUE' column for the specific UCC\n",
    "  filtered_df.loc[filtered_df['UCC'] == negation_ucc, 'VALUE'] *= -1\n",
    "\n",
    "  return filtered_df\n",
    "\n",
    "# Apply the function to the data frames and check the shape \n",
    "income_1_df = filter_and_negate(income_1_df, negation_UCC_value)\n",
    "print(income_1_df.shape)\n",
    "\n",
    "income_2_df = filter_and_negate(income_2_df, negation_UCC_value)\n",
    "print(income_2_df.shape)\n",
    "\n",
    "income_3_df = filter_and_negate(income_3_df, negation_UCC_value)\n",
    "print(income_3_df.shape)\n",
    "\n",
    "income_4_df = filter_and_negate(income_4_df, negation_UCC_value)\n",
    "print(income_4_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18829, 2)\n"
     ]
    }
   ],
   "source": [
    "# Function sums income sources based on participant ID \n",
    "def calculate_total_income(df):\n",
    "\n",
    "#use reset_index to make a hierarchical index a regular column\n",
    "  total_income_df = df.groupby('NEWID')['VALUE'].sum().reset_index() \n",
    "  \n",
    "  # Rename columns in place\n",
    "  total_income_df.columns = ['id', 'total_income']\n",
    "  return total_income_df\n",
    "\n",
    "\n",
    "# Calculate total income for each DataFrame\n",
    "total_income_df1 = calculate_total_income(income_1_df)\n",
    "total_income_df2 = calculate_total_income(income_2_df)\n",
    "total_income_df3 = calculate_total_income(income_3_df)\n",
    "total_income_df4 = calculate_total_income(income_4_df)\n",
    "\n",
    "# Concatenate dataframes to get total income per survey participant\n",
    "total_income_df = pd.concat([total_income_df1, total_income_df2, total_income_df3, total_income_df4], axis = 0)\n",
    "\n",
    "#DF of income over a year\n",
    "print(total_income_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of            id  total_income\n",
      "0     5090604   101692.5000\n",
      "1     5090624    34467.5010\n",
      "2     5090634   155839.9995\n",
      "3     5090664    72695.0001\n",
      "4     5090674    43196.2500\n",
      "...       ...           ...\n",
      "4675  5607961   130770.0000\n",
      "4676  5607981   364462.6290\n",
      "4677  5608001    84775.0005\n",
      "4678  5608051   486724.8864\n",
      "4679  5608061    65240.0010\n",
      "\n",
      "[18829 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(total_income_df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we handle the expenditures data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#  Import data for expenses\n",
    "expense_1_df = pd.read_csv(\"../../data/raw-data/mtbi232.csv\")\n",
    "expense_2_df = pd.read_csv(\"../../data/raw-data/mtbi233.csv\")\n",
    "expense_3_df = pd.read_csv(\"../../data/raw-data/mtbi234.csv\")\n",
    "expense_4_df = pd.read_csv(\"../../data/raw-data/mtbi241.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
