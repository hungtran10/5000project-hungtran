{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Cleaning\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Motivation\n",
    "\n",
    "With this stage, I will merge and clean the data I collected from the Consumer Expenditure Survey via the U.S. Bureau of Labor Statistics. The data is organized in a cryptic way. For example, data on income, expenses, and individuals' demographics are separated into multiple csv files each. Within the csv files, there are numerous encoded variables that need to be deciphered via a data dictionary before I can begin selecting the potentially relevant columns. I will do that for each of the relevant csv files so that there will be a clean data file which I can begin to do exploratory data analysis, and eventually unsupervised and supervised learning on. \n",
    "\n",
    "## Overview of Methods\n",
    "For this portion, I will be utilizing standard data cleaning tasks such as missing value handling, outlier value handling, duplicates removal, merging datasets, one-hot encoding, and filtering for relevant columns. Missing value handling allows for the missing values either to be imputed so the data record can be included, or if there are too many missing values in the records, the record will be dropped altogether. Outlier value handling is important because it helps identify unusual data points that can lead to significant problems in exploratory data analysis, unsupervised learning and supervised learning. Duplicates removal reduce the redundancy in the dataset while column filtering purposely narrows down the dataset to reduce noise. One-hot encoding is the process of converting categorical variables into binary format. For example, the variable 'sex' in the data will have values male and female. Through this process, two new columns will be created, corresponding each of the classes. In each of the resulting columns about male and female, the value will either be 1 (meaning yes) or 0 (meaning no). One-hot encoding is crucial for machine learning algorithms as it improves prediction by allowing ML models to understand and use the data more easily. All of these methods will aid the accuracy of the supervised machine learning models I will train.\n",
    "\n",
    "The expense and income data files are presented on a per instance basis. For example, each expense and income payment for each individual are recorded as an item. In order to make it tidy, I will be aggregating based on the individual identifier 'NEWID' to get total income and expenses in the given year.  Additionally, there will be merging of data frames to form data frames for income, expenses and characteristics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I will be handling the data from the **Consumer Expenditure Survey**. For each of the datasets used from this survey, I had to go through the corresponding data dictionary (stored in an Excel) to select potentially relevant features.\n",
    "We begin with the income data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import data for income\n",
    "income_1_df = pd.read_csv(\"../../data/raw-data/itii232.csv\")\n",
    "income_2_df = pd.read_csv(\"../../data/raw-data/itii233.csv\")\n",
    "income_3_df = pd.read_csv(\"../../data/raw-data/itii234.csv\")\n",
    "income_4_df = pd.read_csv(\"../../data/raw-data/itii241.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.describe of           NEWID  REFMO  REFYR     UCC  PUBFLAG VALUE_  IMPNUM        VALUE\n",
      "0       5090604      1   2023  900030        2    NaN       1  3169.833300\n",
      "1       5090604      1   2023  900030        2    NaN       2  3169.833300\n",
      "2       5090604      1   2023  900030        2    NaN       3  3169.833300\n",
      "3       5090604      1   2023  900030        2    NaN       4  3169.833300\n",
      "4       5090604      1   2023  900030        2    NaN       5  3169.833300\n",
      "...         ...    ...    ...     ...      ...    ...     ...          ...\n",
      "330445  5366911      5   2023  980071        2    NaN       1   820.250000\n",
      "330446  5366911      5   2023  980071        2    NaN       2   250.000000\n",
      "330447  5366911      5   2023  980071        2    NaN       3   100.000000\n",
      "330448  5366911      5   2023  980071        2    NaN       4   294.666667\n",
      "330449  5366911      5   2023  980071        2    NaN       5   160.250000\n",
      "\n",
      "[330450 rows x 8 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Examine one of the dataframes\n",
    "print(income_1_df.describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I filter for the relevant columns in the income dataframes. From the data collection stage, we already know that each of dataframes has 8 columns. We want the following columns:\n",
    "- The variable \"NEWID\" represent the unique identifier for the survey participant. \n",
    "- The feature 'UCC' stands for Universal Classification Code which correpond to goods and services and other things that can be bought or sold. The values under variable \"UCC\" correspond to items that would increases or decreases to the individuals' net worth. \n",
    "- The variable \"VALUE\" indicate the absolute value of the change in net worth. The other 5 variables only represent data reelvant to the survey process so we subset the dataframes for those 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330450, 3)\n",
      "(330840, 3)\n",
      "(322320, 3)\n",
      "(325200, 3)\n"
     ]
    }
   ],
   "source": [
    "income_columns_keep = ['NEWID', 'UCC', 'VALUE']\n",
    "\n",
    "income_1_df = income_1_df[income_columns_keep]\n",
    "print(income_1_df.shape)\n",
    "\n",
    "income_2_df = income_2_df[income_columns_keep]\n",
    "print(income_2_df.shape)\n",
    "\n",
    "income_3_df = income_3_df[income_columns_keep]\n",
    "print(income_3_df.shape)\n",
    "\n",
    "income_4_df = income_4_df[income_columns_keep]\n",
    "print(income_4_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to find the unqiue \"UCC\" values to see if we have to deal with decreases in net worth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[900030, 900170, 900180, 980000, 980071, 800940, 900000, 900160, 900150, 900090, 900190, 900200, 900210, 900120, 900140]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize list that stores all unique values of 'UCC' column\n",
    "all_UCC_unique = []\n",
    "\n",
    "# Function that prints the unique values in a particular column and returns the list\n",
    "def find_unique_UCC_values(df, column_name, unique_UCC_values):\n",
    "\n",
    "  unique_values = df[column_name].unique()\n",
    "  for value in unique_values:\n",
    "    if value not in all_UCC_unique:\n",
    "        unique_UCC_values.append(value)\n",
    "        \n",
    "  \n",
    "find_unique_UCC_values(income_1_df, 'UCC', all_UCC_unique)\n",
    "find_unique_UCC_values(income_2_df, 'UCC', all_UCC_unique)\n",
    "find_unique_UCC_values(income_3_df, 'UCC', all_UCC_unique)\n",
    "find_unique_UCC_values(income_4_df, 'UCC', all_UCC_unique)\n",
    "\n",
    "print(all_UCC_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By referring to the data dictionary, I found that the \"UCC\" values are mostly associated with increases, except for 800940 which represents deductions for social security. \n",
    "There is some overlap between them. For example, 980071 represent income after taxes. Here I want to only focus on pre-tax income for simplicity's sake. Therefore we filter for the following:\n",
    "- 900030: Social Security and railroad retirement income\n",
    "- 900170: Retirement, survivors, disability income\n",
    "- 900180: Interest and dividends\n",
    "- 980000: Income before taxes\n",
    "- 800940: Deductions for Social Security\n",
    "- 900150: Food stamps\n",
    "\n",
    "The following codes correspond to income that is lumped into 980000: Income before taxes\n",
    "- 900160: Self-employment income\n",
    "- 900000: Wages and salaries \n",
    "- 900090: Supplemental security income\n",
    "- 900190: Net room/rental income\n",
    "- 900200: Royalty, estate, trust income\n",
    "- 900210: Other regular income\n",
    "- 900140: Other income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182790, 3)\n",
      "(182475, 3)\n",
      "(178470, 3)\n",
      "(179925, 3)\n"
     ]
    }
   ],
   "source": [
    "income_df_UCC_keep = [900030, 900170, 900180, 980000, 800940, 900150]\n",
    "\n",
    "negation_UCC_value = 800940\n",
    "\n",
    "# Function to filter for the 'UCC' values we want and negate if UCC = 800940\n",
    "def filter_and_negate(df, negation_ucc):\n",
    "\n",
    "  # Filter the DataFrame based on the UCC list\n",
    "  filtered_df = df[df['UCC'].isin(income_df_UCC_keep)]\n",
    "\n",
    "  # Negate the 'VALUE' column for the specific UCC\n",
    "  filtered_df.loc[filtered_df['UCC'] == negation_ucc, 'VALUE'] *= -1\n",
    "\n",
    "  return filtered_df\n",
    "\n",
    "# Apply the function to the data frames and check the shape \n",
    "income_1_df = filter_and_negate(income_1_df, negation_UCC_value)\n",
    "print(income_1_df.shape)\n",
    "\n",
    "income_2_df = filter_and_negate(income_2_df, negation_UCC_value)\n",
    "print(income_2_df.shape)\n",
    "\n",
    "income_3_df = filter_and_negate(income_3_df, negation_UCC_value)\n",
    "print(income_3_df.shape)\n",
    "\n",
    "income_4_df = filter_and_negate(income_4_df, negation_UCC_value)\n",
    "print(income_4_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cleaned income dataframes, we aggregate each to find the total income per year per individual and merge all the dataframes to get the master income dataframe. Then we remove duplicates and filter outliers. I have decided to only eliminate outliers for this dataframe because I am assuming no one will be spending significantly more than they earn. Therefore, eliminating outliers for the income dataframe should be adequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17382, 2)\n"
     ]
    }
   ],
   "source": [
    "# Function sums income sources based on participant ID \n",
    "def calculate_total_income(df):\n",
    "\n",
    "# Use reset_index to make a hierarchical index a regular column\n",
    "  total_income_df = df.groupby('NEWID')['VALUE'].sum().reset_index() \n",
    "  \n",
    "  # Rename columns in place\n",
    "  total_income_df.columns = ['id', 'total_income']\n",
    "  return total_income_df\n",
    "\n",
    "\n",
    "# Calculate total income for each DataFrame\n",
    "total_income_df1 = calculate_total_income(income_1_df)\n",
    "total_income_df2 = calculate_total_income(income_2_df)\n",
    "total_income_df3 = calculate_total_income(income_3_df)\n",
    "total_income_df4 = calculate_total_income(income_4_df)\n",
    "\n",
    "# Concatenate dataframes to get total income per survey participant\n",
    "total_income_df = pd.concat([total_income_df1, total_income_df2, total_income_df3, total_income_df4], axis = 0)\n",
    "\n",
    "# Remove duplicates based on 'id' column\n",
    "total_income_df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "\n",
    "# Remove outliers\n",
    "# Calculate IQR and identify outliers\n",
    "Q1 = total_income_df['total_income'].quantile(0.25)\n",
    "Q3 = total_income_df['total_income'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers\n",
    "total_income_df = total_income_df[(total_income_df['total_income'] >= lower_bound) & (total_income_df['total_income'] <= upper_bound)]\n",
    "\n",
    "# Dataframe of income over a year\n",
    "print(total_income_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of            id  total_income\n",
      "0     5090604   101692.5000\n",
      "1     5090624    34467.5010\n",
      "2     5090634   155839.9995\n",
      "3     5090664    72695.0001\n",
      "4     5090674    43196.2500\n",
      "...       ...           ...\n",
      "4673  5607931   222792.4995\n",
      "4674  5607951    54740.0010\n",
      "4675  5607961   130770.0000\n",
      "4677  5608001    84775.0005\n",
      "4679  5608061    65240.0010\n",
      "\n",
      "[17382 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(total_income_df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we handle the expenditures data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import data for expenses\n",
    "expense_1_df = pd.read_csv(\"../../data/raw-data/mtbi232.csv\")\n",
    "expense_2_df = pd.read_csv(\"../../data/raw-data/mtbi233.csv\")\n",
    "expense_3_df = pd.read_csv(\"../../data/raw-data/mtbi234.csv\")\n",
    "expense_4_df = pd.read_csv(\"../../data/raw-data/mtbi241.csv\")\n",
    "\n",
    "# Subset expense Data Frames for the relevant columns\n",
    "expense_columns_keep = ['NEWID', 'SEQNO', 'UCC', 'COST']\n",
    "expense_1_df = expense_1_df[expense_columns_keep]\n",
    "expense_2_df = expense_2_df[expense_columns_keep]\n",
    "expense_3_df = expense_3_df[expense_columns_keep]\n",
    "expense_4_df = expense_4_df[expense_columns_keep]\n",
    "\n",
    "expense_df = pd.concat([expense_1_df, expense_2_df, expense_3_df, expense_4_df], axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By consulting the data dictionary, I located the specific files that listed the online purchases of tangible goods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/dt8s0j7j0glg1jfrlpndm0lw0000gn/T/ipykernel_84138/3622570946.py:2: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  specific_expense_df1 = pd.read_csv(\"../../data/raw-data/apb23.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Import csv files for online purchases\n",
    "specific_expense_df1 = pd.read_csv(\"../../data/raw-data/apb23.csv\")\n",
    "specific_expense_df2 = pd.read_csv(\"../../data/raw-data/eqb23.csv\")\n",
    "specific_expense_df3 = pd.read_csv(\"../../data/raw-data/mis23.csv\")\n",
    "specific_expense_df4 = pd.read_csv(\"../../data/raw-data/ovb23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['QYEAR', 'NEWID', 'SEQNO', 'ALCNO', 'REC_ORIG', 'MINAPPLY', 'MINA_PLY',\n",
      "       'GFTCMIN', 'GFTCMIN_', 'MIN_MO', 'MIN_MO_', 'MINPURX', 'MINPURX_',\n",
      "       'MINRENTX', 'MINR_NTX', 'MNAPPL1', 'MNAPPL1_', 'MNAPPL2', 'MNAPPL2_',\n",
      "       'MNAPPL3', 'MNAPPL3_', 'MNAPPL4', 'MNAPPL4_', 'MNAPPL5', 'MNAPPL5_',\n",
      "       'MNAPPL6', 'MNAPPL6_', 'MNAPPL7', 'MNAPPL7_', 'MNAPPL8', 'MNAPPL8_',\n",
      "       'MNAPPL9', 'MNAPPL9_', 'INSTLSCR', 'INST_SCR', 'INSTLLEX', 'INST_LEX',\n",
      "       'APBPURCH'],\n",
      "      dtype='object')\n",
      "Index(['QYEAR', 'NEWID', 'SEQNO', 'ALCNO', 'REC_ORIG', 'APPRPRYB', 'APPR_RYB',\n",
      "       'SRVCMOB', 'SRVCMOB_', 'REPAIRX', 'REPAIRX_', 'APPRPB1', 'APPRPB1_',\n",
      "       'APPRPB2', 'APPRPB2_', 'APPRPB3', 'APPRPB3_', 'APPRPB4', 'APPRPB4_',\n",
      "       'APPRPB5', 'APPRPB5_', 'APPRPB6', 'APPRPB6_', 'APPRPB7', 'APPRPB7_',\n",
      "       'APPRPB8', 'APPRPB8_', 'APPRPB9', 'EQBPURCH', 'APPRPB9_'],\n",
      "      dtype='object')\n",
      "Index(['QYEAR', 'NEWID', 'SEQNO', 'ALCNO', 'REC_ORIG', 'MISCCODE', 'MISC_ODE',\n",
      "       'MISCMO', 'MISCMO_', 'MISCGFTC', 'MISC_FTC', 'MISCEXPX', 'MISC_XPX',\n",
      "       'MISCDE1', 'MISCDE1_', 'MISCDE2', 'MISCDE2_', 'MISCDE3', 'MISCDE3_',\n",
      "       'MISCDE4', 'MISCDE4_', 'MISCDE5', 'MISCDE5_', 'MISCDE6', 'MISCDE6_',\n",
      "       'MISCDE7', 'MISCDE7_', 'MISCDE8', 'MISCDE8_', 'MISCDE9', 'MISPURCH',\n",
      "       'MISCDE9_'],\n",
      "      dtype='object')\n",
      "Index(['QYEAR', 'NEWID', 'SEQNO', 'ALCNO', 'REC_ORIG', 'VEHICYB', 'VEHICYB_',\n",
      "       'VEHICYR', 'VEHICYR_', 'VEHBSNZ', 'VEHBSNZ_', 'VEHNEWU', 'VEHNEWU_',\n",
      "       'VEHGFTC', 'VEHGFTC_', 'VEHPURMO', 'VEHP_RMO', 'VEHPURYR', 'VEHP_RYR',\n",
      "       'VFINSTAT', 'VFIN_TAT', 'TRADEX', 'TRADEX_', 'NETPURX', 'NETPURX_',\n",
      "       'DNPAYMTX', 'DNPA_MTX', 'PRINCIPX', 'PRIN_IPX', 'VEHQPMT', 'VEHQPMT_',\n",
      "       'PMT1MO', 'PMT1MO_', 'PMT1YR', 'PMT1YR_', 'PAYMENTX', 'PAYM_NTX',\n",
      "       'QINTRSTZ', 'QINT_STZ', 'QLOANM1Q', 'QLOA_M1Q', 'QBALNM1X', 'QBAL_M1X',\n",
      "       'QVINTM1X', 'QVIN_M1X', 'QVPRIM1X', 'QVPR_M1X', 'QLOANM2Q', 'QLOA_M2Q',\n",
      "       'QBALNM2X', 'QBAL_M2X', 'QVINTM2X', 'QVIN_M2X', 'QVPRIM2X', 'QVPR_M2X',\n",
      "       'QLOANM3Q', 'QLOA_M3Q', 'QBALNM3X', 'QBAL_M3X', 'QVINTM3X', 'QVIN_M3X',\n",
      "       'QVPRIM3X', 'QVPR_M3X', 'QTRADEX', 'QTRADEX_', 'QREIMBRZ', 'QREI_BRZ',\n",
      "       'QADITR1X', 'QADI_R1X', 'QADITR2X', 'QADI_R2X', 'QADITR3X', 'QADI_R3X',\n",
      "       'QDNPYMTX', 'QDNP_MTX', 'VEHEQTLN', 'VEHE_TLN', 'VEHICIB', 'VEHICIB_',\n",
      "       'MAKE', 'MAKE_', 'OVBPURCH', 'VFINANCE', 'FUELTYPE', 'FUEL_YPE',\n",
      "       'VPURINDV', 'VPUR_NDV', 'VINTRATE', 'VINT_ATE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(specific_expense_df1.columns)\n",
    "print(specific_expense_df2.columns)\n",
    "print(specific_expense_df3.columns)\n",
    "print(specific_expense_df4.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the specific purchase Data Frames for the relevant columns so we get four dataframes of online expenses.\n",
    "- 'SEQNO' is the identifier variable for the purchases and can be used to merge with expense data frames. \n",
    "- 'APBPURCH' tells us if this item was purchased online or in-person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "apb_columns_keep = ['NEWID', 'SEQNO', 'APBPURCH']\n",
    "online_expense_df1 = specific_expense_df1[apb_columns_keep]\n",
    "online_expense_df1_subset = online_expense_df1.loc[online_expense_df1['APBPURCH'] == 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'EQBPURCH' tells us if this item was bought online or in-person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqb_columns_keep = ['NEWID', 'SEQNO', 'EQBPURCH']\n",
    "online_expense_df2 = specific_expense_df2[eqb_columns_keep]\n",
    "online_expense_df2_subset = online_expense_df2.loc[online_expense_df2['EQBPURCH'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'MISPURCH' tells us if this item was bought online or in-person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_columns_keep = ['NEWID', 'SEQNO', 'MISPURCH']\n",
    "online_expense_df3 = specific_expense_df3[mis_columns_keep]\n",
    "online_expense_df3_subset = online_expense_df3.loc[online_expense_df3['MISPURCH'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'OVBPURCH' tells us if this item was bought online or in-person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovb_columns_keep = ['NEWID', 'SEQNO', 'OVBPURCH']\n",
    "online_expense_df4 = specific_expense_df4[ovb_columns_keep]\n",
    "online_expense_df4_subset = online_expense_df4.loc[online_expense_df4['OVBPURCH'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the mini-dataframes to get the the online_expense_df which we will merge with the expense_df to see the dollar amount of the online purchase. Then, we aggregate based upon 'NEWID' primary key to calculate the total expense per person, total online expense and the online spending percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online Expense Columns:  Index(['NEWID', 'SEQNO', 'Is_Online'], dtype='object')\n",
      "Merged Expense Columns:  Index(['NEWID', 'SEQNO', 'UCC', 'COST', 'Is_Online'], dtype='object')\n",
      "Total Expense Columns:  Index(['id', 'total_expense', 'online_expense', 'online_percentage',\n",
      "       'bought_online'],\n",
      "      dtype='object')\n",
      "(18871, 5)\n"
     ]
    }
   ],
   "source": [
    "#Concatenate all online expense df\n",
    "online_expense_df = pd.concat([online_expense_df1_subset, online_expense_df2_subset, online_expense_df3_subset, online_expense_df4_subset], axis = 0)\n",
    "\n",
    "# Filter for relevant columns\n",
    "online_expense_df = online_expense_df[['NEWID', 'SEQNO']]\n",
    "\n",
    "# Add a column with imputed constant values of 1 for later merging so we know the expenses are online \n",
    "online_expense_df['Is_Online'] = 1\n",
    "print(\"Online Expense Columns: \",online_expense_df.columns)\n",
    "\n",
    "# Merge the two DataFrames based on 'NEWID' and 'SEQNO'\n",
    "merged_expense_df = expense_df.merge(online_expense_df, on=['NEWID', 'SEQNO'], how='left')\n",
    "print(\"Merged Expense Columns: \", merged_expense_df.columns)\n",
    "\n",
    "\n",
    "# Calculate total expenses, online expenses, and online percentage for each individual\n",
    "# Group by 'NEWID' column, and perform 3 aggregate functions\n",
    "total_expense_df = merged_expense_df.groupby('NEWID').agg(\n",
    "\n",
    "    id=('NEWID', 'first'),\n",
    "\n",
    "    #create a new column, and calculates the sum of 'COST'\n",
    "    total_expense=('COST', 'sum'), \n",
    "\n",
    "    #filter COST column to include only rows where Is_Online is 1 ( online purchases)\n",
    "    online_expense=('COST', lambda x: x[merged_expense_df['Is_Online'] == 1].sum()), \n",
    "\n",
    "    #filter COST column for online purchases and claculates that as a % of total\n",
    "    online_percentage=('COST', lambda x: x[merged_expense_df['Is_Online'] == 1].sum() / x.sum() * 100)\n",
    ")\n",
    "\n",
    "# Add a column 'bought_online' to indicate whether the person bought something online\n",
    "total_expense_df['bought_online'] = (total_expense_df['online_expense'] > 0).astype(int)\n",
    "\n",
    "# Drop duplicates\n",
    "total_expense_df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "\n",
    "print(\"Total Expense Columns: \", total_expense_df.columns)\n",
    "print(total_expense_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to handling characteristics data. This requires a lot of referring to the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data for individual characteristics\n",
    "characteristics_df_1 = pd.read_csv(\"../../data/raw-data/memi232.csv\")\n",
    "characteristics_df_2 = pd.read_csv(\"../../data/raw-data/memi233.csv\")\n",
    "characteristics_df_3 = pd.read_csv(\"../../data/raw-data/memi234.csv\")\n",
    "characteristics_df_4 = pd.read_csv(\"../../data/raw-data/memi241.csv\")\n",
    "\n",
    "# Filter for  relevant columns\n",
    "characteristics_columns_keep = [\"NEWID\", \"AGE\", \"ARM_FORC\", \"EARNTYPE\", \"EDUCA\", \"INCWEEKQ\", \n",
    "                                \"INDRETX\", \"JSSDEDX\", \"JSSDEDXM\", \"MARITAL\", \"MEMBRACE\", \"RC_ASIAN\", \"RC_BLACK\", \"RC_DK\", \"RC_NATAM\",\n",
    "                                \"RC_OTHER\", \"RC_PACIL\", \"RC_WHITE\", \"SEX\", \"SOCRRX\"]\n",
    "\n",
    "characteristics_df_1 = characteristics_df_1[characteristics_columns_keep]\n",
    "characteristics_df_2 = characteristics_df_2[characteristics_columns_keep]\n",
    "characteristics_df_3 = characteristics_df_3[characteristics_columns_keep]\n",
    "characteristics_df_4 = characteristics_df_4[characteristics_columns_keep]\n",
    "\n",
    "# Combine all of them\n",
    "characteristics_df = pd.concat([characteristics_df_1, characteristics_df_2, characteristics_df_3, characteristics_df_4], axis = 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "characteristics_df.rename(columns={\"NEWID\": \"id\", \"AGE\": \"age\", \"ARM_FORC\": \"is_military\",  \"EARNTYPE\": \"earning_type\", \"EDUCA\": \"highest_ed_completed\", \n",
    "                                   \"INCWEEKQ\": \"num_weeks_worked_in_last_yr\", \"INDRETX\": \"deposited_money_in_retirement_this_yr\", \n",
    "                                   \"JSSDEDX\": \"income_into_SS_this_yr\", \"JSSDEDXM\": \"SS_payments_received_this_yr\", \"MARITAL\": \"marital_status\", \n",
    "                                   \"MEMBRACE\": \"race\", \"RC_ASIAN\": \"is_asian\", \"RC_BLACK\": \"is_black\", \"RC_DK\": \"race_unknown\", \n",
    "                                   \"RC_NATAM\": \"is_native_american\", \"RC_OTHER\": \"is_other_race\", \"RC_PACIL\": \"is_pacific_islander\", \n",
    "                                   \"RC_WHITE\": \"is_white\", \"SEX\": \"sex\", \"SOCRRX\": \"SS_and_railroad_retirement_income_received\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some data cleaning for the categorical variables and one-hot encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning for one-hot encoding \n",
    "# Assign not military to 0\n",
    "characteristics_df[\"is_military\"] = characteristics_df['is_military'].replace(2, 0)\n",
    "# Assign female to 0\n",
    "characteristics_df[\"sex\"] = characteristics_df['sex'].replace(2, 0)\n",
    "\n",
    "# One-hot encode and add columns back to the dataframe\n",
    "# Earning Type\n",
    "earning_type_dummies = pd.get_dummies(characteristics_df['earning_type'], prefix='', prefix_sep='', dtype=int)\n",
    "earning_type_dummies.rename(columns={\n",
    "    '1.0': 'full_time_1_yr', \n",
    "    '2.0': 'part_time_1_yr', \n",
    "    '3.0': 'full_time_part_yr', \n",
    "    '4.0': 'part_time_part_yr'\n",
    "}, inplace=True)\n",
    "characteristics_df = pd.concat([characteristics_df, earning_type_dummies], axis=1)\n",
    "\n",
    "# Highest Education Completed\n",
    "highest_ed_dummies = pd.get_dummies(characteristics_df['highest_ed_completed'], prefix='', prefix_sep='', dtype=int)\n",
    "highest_ed_dummies.rename(columns={\n",
    "    '1.0': 'no_school_completed', \n",
    "    '2.0': 'grades_1-8_completed', \n",
    "    '3.0': 'high_school_no_degree', \n",
    "    '4.0': 'high_school_grad',\n",
    "    '5.0': 'some_college_no_degree',\n",
    "    '6.0': 'associates_degree',\n",
    "    '7.0': 'bachelors_degree',\n",
    "    '8.0': 'graduate_degree'\n",
    "}, inplace=True)\n",
    "characteristics_df = pd.concat([characteristics_df, highest_ed_dummies], axis=1)\n",
    "\n",
    "# Marital Status\n",
    "marital_dummies = pd.get_dummies(characteristics_df['marital_status'], prefix='', prefix_sep='', dtype=int)\n",
    "marital_dummies.rename(columns={\n",
    "    '1': 'is_married', \n",
    "    '2': 'is_widowed', \n",
    "    '3': 'is_divorced', \n",
    "    '4': 'is_separated',\n",
    "    '5': 'is_single'\n",
    "}, inplace=True)\n",
    "characteristics_df = pd.concat([characteristics_df, marital_dummies], axis=1)\n",
    "\n",
    "# Sex\n",
    "sex_dummies = pd.get_dummies(characteristics_df['sex'], prefix='', prefix_sep='', dtype=int)\n",
    "sex_dummies.rename(columns={\n",
    "    '1': 'is_male', \n",
    "    '0': 'is_female'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "characteristics_df = pd.concat([characteristics_df, sex_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'age', 'is_military', 'earning_type', 'highest_ed_completed',\n",
      "       'num_weeks_worked_in_last_yr', 'deposited_money_in_retirement_this_yr',\n",
      "       'income_into_SS_this_yr', 'SS_payments_received_this_yr',\n",
      "       'marital_status', 'race', 'is_asian', 'is_black', 'race_unknown',\n",
      "       'is_native_american', 'is_other_race', 'is_pacific_islander',\n",
      "       'is_white', 'sex', 'SS_and_railroad_retirement_income_received',\n",
      "       'full_time_1_yr', 'part_time_1_yr', 'full_time_part_yr',\n",
      "       'part_time_part_yr', 'no_school_completed', 'grades_1-8_completed',\n",
      "       'high_school_no_degree', 'high_school_grad', 'some_college_no_degree',\n",
      "       'associates_degree', 'bachelors_degree', 'graduate_degree',\n",
      "       'is_married', 'is_widowed', 'is_divorced', 'is_separated', 'is_single',\n",
      "       'is_female', 'is_male'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(characteristics_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View values per column about race\n",
    "# Legend: White - 1, Black - 2, Native American - 3, Asian - 4, Pacific Islander - 5, Other - 6, Unknown - 7\n",
    "race_columns = [\"is_asian\", \"is_black\", \"race_unknown\", \"is_native_american\", \"is_other_race\", \"is_pacific_islander\", \"is_white\"]\n",
    "\n",
    "# One-Hot Encode Race Columns\n",
    "# Replace numerical values for one-hot-encoding\n",
    "characteristics_df[\"is_asian\"] = characteristics_df['is_asian'].replace(4, 1)\n",
    "characteristics_df[\"is_black\"] = characteristics_df['is_black'].replace(2, 1)\n",
    "characteristics_df[\"race_unknown\"] = characteristics_df['race_unknown'].replace(7, 1)\n",
    "characteristics_df[\"is_native_american\"] = characteristics_df['is_native_american'].replace(3, 1)\n",
    "characteristics_df[\"is_other_race\"] = characteristics_df['is_other_race'].replace(6, 1)\n",
    "characteristics_df[\"is_pacific_islander\"] = characteristics_df['is_pacific_islander'].replace(5, 1)\n",
    "\n",
    "# Replace NaN values with 7 (unknown race)\n",
    "characteristics_df[race_columns] = characteristics_df[race_columns].fillna(7)\n",
    "\n",
    "# Rename earning_type categories and replace numbers with categorical values\n",
    "earning_dict = {1: 'full-time 1 year', 2: 'part-time 1 year', 3: 'full-time part year', 4: 'part-time part year'}\n",
    "characteristics_df['earning_type'] = characteristics_df['earning_type'].replace(earning_dict)\n",
    "\n",
    "# Rename race categories and replace numbers with categorical values\n",
    "race_dict = {1: 'white', 2: 'black', 3: 'native american', 4: 'asian', 5: 'pacific islander', 6: 'other', 7: 'unknown'}\n",
    "characteristics_df['race'] = characteristics_df['race'].replace(race_dict)\n",
    "\n",
    "# Rename marital_status categories and replace numbers with categorical values\n",
    "marital_dict = {1: 'married', 2: 'widowed', 3: 'divorced', 4: 'separated', 5: 'never married'}\n",
    "characteristics_df['marital_status'] = characteristics_df['marital_status'].replace(marital_dict)\n",
    "\n",
    "# Rename highest_ed_completed categories and replace numbers with categorical values\n",
    "ed_dict = {1: 'no school completed', 2: 'grades 1-8', 3: 'high school no degree', 4: 'high school grad', 5: 'some college no degree', \n",
    "           6: 'associates degree', 7: 'bachelors degree', 8: 'graduate degree'}\n",
    "characteristics_df['highest_ed_completed'] = characteristics_df['highest_ed_completed'].replace(ed_dict)\n",
    "\n",
    "# Rename sex categories and replace numbers with categorical values\n",
    "sex_dict = {0: 'female', 1: 'male'}\n",
    "characteristics_df['sex'] = characteristics_df['sex'].replace(sex_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'age', 'is_military', 'earning_type', 'highest_ed_completed',\n",
      "       'num_weeks_worked_in_last_yr', 'deposited_money_in_retirement_this_yr',\n",
      "       'income_into_SS_this_yr', 'SS_payments_received_this_yr',\n",
      "       'marital_status', 'race', 'is_asian', 'is_black', 'race_unknown',\n",
      "       'is_native_american', 'is_other_race', 'is_pacific_islander',\n",
      "       'is_white', 'sex', 'SS_and_railroad_retirement_income_received',\n",
      "       'full_time_1_yr', 'part_time_1_yr', 'full_time_part_yr',\n",
      "       'part_time_part_yr', 'no_school_completed', 'grades_1-8_completed',\n",
      "       'high_school_no_degree', 'high_school_grad', 'some_college_no_degree',\n",
      "       'associates_degree', 'bachelors_degree', 'graduate_degree',\n",
      "       'is_married', 'is_widowed', 'is_divorced', 'is_separated', 'is_single',\n",
      "       'is_female', 'is_male'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(characteristics_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge the total income, expenses, and characteristics dataframe with one another to form the final dataframe, ces_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'total_expense', 'online_expense', 'online_percentage',\n",
      "       'bought_online', 'total_income', 'age', 'is_military', 'earning_type',\n",
      "       'highest_ed_completed', 'num_weeks_worked_in_last_yr',\n",
      "       'deposited_money_in_retirement_this_yr', 'income_into_SS_this_yr',\n",
      "       'SS_payments_received_this_yr', 'marital_status', 'race', 'is_asian',\n",
      "       'is_black', 'race_unknown', 'is_native_american', 'is_other_race',\n",
      "       'is_pacific_islander', 'is_white', 'sex',\n",
      "       'SS_and_railroad_retirement_income_received', 'full_time_1_yr',\n",
      "       'part_time_1_yr', 'full_time_part_yr', 'part_time_part_yr',\n",
      "       'no_school_completed', 'grades_1-8_completed', 'high_school_no_degree',\n",
      "       'high_school_grad', 'some_college_no_degree', 'associates_degree',\n",
      "       'bachelors_degree', 'graduate_degree', 'is_married', 'is_widowed',\n",
      "       'is_divorced', 'is_separated', 'is_single', 'is_female', 'is_male'],\n",
      "      dtype='object')\n",
      "<bound method NDFrame.head of             id  total_expense  online_expense  online_percentage  \\\n",
      "0      5090604  184019.000000             0.0                0.0   \n",
      "1      5090604  184019.000000             0.0                0.0   \n",
      "2      5090624    4616.000000             0.0                0.0   \n",
      "3      5090634  150661.999999             0.0                0.0   \n",
      "4      5090664   57142.999999             0.0                0.0   \n",
      "...        ...            ...             ...                ...   \n",
      "39906  5607961   13624.000000             0.0                0.0   \n",
      "39907  5607961   13624.000000             0.0                0.0   \n",
      "39908  5607961   13624.000000             0.0                0.0   \n",
      "39909  5608001   12992.000000             0.0                0.0   \n",
      "39910  5608061   58554.000000             0.0                0.0   \n",
      "\n",
      "       bought_online  total_income  age  is_military      earning_type  \\\n",
      "0                  0   101692.5000   74          NaN               NaN   \n",
      "1                  0   101692.5000   72          NaN               NaN   \n",
      "2                  0    34467.5010   87          NaN               NaN   \n",
      "3                  0   155839.9995   58          0.0  full-time 1 year   \n",
      "4                  0    72695.0001   55          0.0  full-time 1 year   \n",
      "...              ...           ...  ...          ...               ...   \n",
      "39906              0   130770.0000   77          NaN               NaN   \n",
      "39907              0   130770.0000   64          0.0               NaN   \n",
      "39908              0   130770.0000   43          0.0               NaN   \n",
      "39909              0    84775.0005   37          0.0  full-time 1 year   \n",
      "39910              0    65240.0010   67          NaN               NaN   \n",
      "\n",
      "        highest_ed_completed  ...  associates_degree  bachelors_degree  \\\n",
      "0            graduate degree  ...                  0                 0   \n",
      "1          associates degree  ...                  1                 0   \n",
      "2           high school grad  ...                  0                 0   \n",
      "3           bachelors degree  ...                  0                 1   \n",
      "4      high school no degree  ...                  0                 0   \n",
      "...                      ...  ...                ...               ...   \n",
      "39906       high school grad  ...                  0                 0   \n",
      "39907       high school grad  ...                  0                 0   \n",
      "39908       high school grad  ...                  0                 0   \n",
      "39909        graduate degree  ...                  0                 0   \n",
      "39910       high school grad  ...                  0                 0   \n",
      "\n",
      "       graduate_degree  is_married is_widowed is_divorced  is_separated  \\\n",
      "0                    1           1          0           0             0   \n",
      "1                    0           1          0           0             0   \n",
      "2                    0           0          0           1             0   \n",
      "3                    0           0          0           1             0   \n",
      "4                    0           1          0           0             0   \n",
      "...                ...         ...        ...         ...           ...   \n",
      "39906                0           0          0           1             0   \n",
      "39907                0           0          0           1             0   \n",
      "39908                0           0          0           0             0   \n",
      "39909                1           0          0           0             0   \n",
      "39910                0           0          0           0             1   \n",
      "\n",
      "       is_single  is_female  is_male  \n",
      "0              0          0        1  \n",
      "1              0          1        0  \n",
      "2              0          1        0  \n",
      "3              0          1        0  \n",
      "4              0          0        1  \n",
      "...          ...        ...      ...  \n",
      "39906          0          0        1  \n",
      "39907          0          1        0  \n",
      "39908          1          0        1  \n",
      "39909          1          1        0  \n",
      "39910          0          1        0  \n",
      "\n",
      "[39911 rows x 44 columns]>\n"
     ]
    }
   ],
   "source": [
    "ces_df = total_expense_df.merge(total_income_df, on='id', how='inner')\n",
    "ces_df = ces_df.merge(characteristics_df, on='id',how='left')\n",
    "\n",
    "print(ces_df.columns)\n",
    "print(ces_df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for nulls in each column. After examining the null counts and their respective columns, there seem to be quite a few null values and several columns. I will drop them as there is a significant chunk of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                                0\n",
      "total_expense                                     0\n",
      "online_expense                                    0\n",
      "online_percentage                                 0\n",
      "bought_online                                     0\n",
      "total_income                                      0\n",
      "age                                               0\n",
      "is_military                                   15826\n",
      "earning_type                                  18886\n",
      "highest_ed_completed                           6066\n",
      "num_weeks_worked_in_last_yr                    6067\n",
      "deposited_money_in_retirement_this_yr         35028\n",
      "income_into_SS_this_yr                        18855\n",
      "SS_payments_received_this_yr                  18855\n",
      "marital_status                                    0\n",
      "race                                              0\n",
      "is_asian                                          0\n",
      "is_black                                          0\n",
      "race_unknown                                      0\n",
      "is_native_american                                0\n",
      "is_other_race                                     0\n",
      "is_pacific_islander                               0\n",
      "is_white                                          0\n",
      "sex                                               0\n",
      "SS_and_railroad_retirement_income_received     6067\n",
      "full_time_1_yr                                    0\n",
      "part_time_1_yr                                    0\n",
      "full_time_part_yr                                 0\n",
      "part_time_part_yr                                 0\n",
      "no_school_completed                               0\n",
      "grades_1-8_completed                              0\n",
      "high_school_no_degree                             0\n",
      "high_school_grad                                  0\n",
      "some_college_no_degree                            0\n",
      "associates_degree                                 0\n",
      "bachelors_degree                                  0\n",
      "graduate_degree                                   0\n",
      "is_married                                        0\n",
      "is_widowed                                        0\n",
      "is_divorced                                       0\n",
      "is_separated                                      0\n",
      "is_single                                         0\n",
      "is_female                                         0\n",
      "is_male                                           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(ces_df.isnull().sum())\n",
    "ces_df.drop(columns=['is_military', 'num_weeks_worked_in_last_yr', 'deposited_money_in_retirement_this_yr', \n",
    "                              'income_into_SS_this_yr', 'SS_payments_received_this_yr', 'SS_and_railroad_retirement_income_received'],axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                            0\n",
      "total_expense                 0\n",
      "online_expense                0\n",
      "online_percentage             0\n",
      "bought_online                 0\n",
      "total_income                  0\n",
      "age                           0\n",
      "earning_type              18886\n",
      "highest_ed_completed       6066\n",
      "marital_status                0\n",
      "race                          0\n",
      "is_asian                      0\n",
      "is_black                      0\n",
      "race_unknown                  0\n",
      "is_native_american            0\n",
      "is_other_race                 0\n",
      "is_pacific_islander           0\n",
      "is_white                      0\n",
      "sex                           0\n",
      "full_time_1_yr                0\n",
      "part_time_1_yr                0\n",
      "full_time_part_yr             0\n",
      "part_time_part_yr             0\n",
      "no_school_completed           0\n",
      "grades_1-8_completed          0\n",
      "high_school_no_degree         0\n",
      "high_school_grad              0\n",
      "some_college_no_degree        0\n",
      "associates_degree             0\n",
      "bachelors_degree              0\n",
      "graduate_degree               0\n",
      "is_married                    0\n",
      "is_widowed                    0\n",
      "is_divorced                   0\n",
      "is_separated                  0\n",
      "is_single                     0\n",
      "is_female                     0\n",
      "is_male                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# New null values\n",
    "print(ces_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns 'total_income', 'online_percentage' and 'total_expense' both have negative values. Negative income can suggest a net loss in net worth such as expenses exceeding income. Similarly, a negative online spending percentage is spending beyond means. However, there are only 54 negative total income and 8 negative online percentage values so let's drop the rows where that occurs.\n",
    "\n",
    "We also want to remove outliers in the dataset based on income. The rationale is that income is the main driver for expenses and therefore online spending. In other words, those who spend more online will on average have a higher income "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       total_expense  online_expense  online_percentage   total_income  \\\n",
      "count   3.991100e+04    39911.000000       39911.000000   39911.000000   \n",
      "mean    1.073045e+05      124.981985           0.090174  123548.701514   \n",
      "std     1.226776e+05     3402.473749           1.629428   76393.490978   \n",
      "min    -6.417860e+05        0.000000         -18.103448  -47375.425500   \n",
      "25%     2.083075e+04        0.000000           0.000000   64980.000000   \n",
      "50%     8.383250e+04        0.000000           0.000000  109979.249400   \n",
      "75%     1.463160e+05        0.000000           0.000000  169692.499500   \n",
      "max     2.077078e+06   156872.000000          85.916254  349275.721500   \n",
      "\n",
      "                age  \n",
      "count  39911.000000  \n",
      "mean      41.987447  \n",
      "std       23.894380  \n",
      "min        0.000000  \n",
      "25%       22.000000  \n",
      "50%       42.000000  \n",
      "75%       62.000000  \n",
      "max       87.000000  \n",
      "Number of negative values in 'total_income': 54\n",
      "Number of negative values in 'online_expense': 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/dt8s0j7j0glg1jfrlpndm0lw0000gn/T/ipykernel_84138/1384561808.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ces_df_filtered['income_category'] = ces_df_filtered['total_income'].apply(income_category)\n"
     ]
    }
   ],
   "source": [
    "df_with_numerical_col = ces_df[['total_expense', 'online_expense', 'online_percentage', 'total_income', 'age']]\n",
    "print(df_with_numerical_col.describe())\n",
    "\n",
    "# Count negative values in each column\n",
    "neg_total_income = (ces_df['total_income'] < 0).sum()\n",
    "neg_online_expense = (ces_df['online_expense'] < 0).sum()\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of negative values in 'total_income':\", neg_total_income)\n",
    "print(\"Number of negative values in 'online_expense':\", neg_online_expense)\n",
    "\n",
    "# Drop negative values\n",
    "ces_df.drop(ces_df[ces_df['total_income']<0].index, inplace=True)\n",
    "ces_df.drop(ces_df[ces_df['total_expense']<0].index, inplace=True)\n",
    "ces_df.drop(ces_df[ces_df['online_expense']<0].index, inplace=True)\n",
    "\n",
    "# Remove income and online expense outliers\n",
    "\n",
    "#Set Q1 and Q3 for interquartile range\n",
    "Q1_income = ces_df['total_income'].quantile(0.25)\n",
    "Q3_income = ces_df['total_income'].quantile(0.75)\n",
    "IQR_income = Q3_income - Q1_income\n",
    "\n",
    "# Define outlier thresholds\n",
    "lower_bound_income = Q1_income - 1.5 * IQR_income\n",
    "upper_bound_income = Q3_income + 1.5 * IQR_income\n",
    "\n",
    "\n",
    "# Remove outliers\n",
    "ces_df_filtered = ces_df[(ces_df['total_income'] >= lower_bound_income) & (ces_df['total_income'] <= upper_bound_income)]\n",
    "\n",
    "# Create a new column 'Income_Category' based on 'total_income' for classification task\n",
    "def income_category(income):\n",
    "    if income <= 50000:\n",
    "        return 'Low'\n",
    "    elif 50000 < income <= 100000:\n",
    "        return 'Middle'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "ces_df_filtered['income_category'] = ces_df_filtered['total_income'].apply(income_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income_category\n",
      "High      21335\n",
      "Middle    11239\n",
      "Low        6683\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(ces_df_filtered['income_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'total_expense', 'online_expense', 'online_percentage',\n",
      "       'bought_online', 'total_income', 'age', 'earning_type',\n",
      "       'highest_ed_completed', 'marital_status', 'race', 'is_asian',\n",
      "       'is_black', 'race_unknown', 'is_native_american', 'is_other_race',\n",
      "       'is_pacific_islander', 'is_white', 'sex', 'full_time_1_yr',\n",
      "       'part_time_1_yr', 'full_time_part_yr', 'part_time_part_yr',\n",
      "       'no_school_completed', 'grades_1-8_completed', 'high_school_no_degree',\n",
      "       'high_school_grad', 'some_college_no_degree', 'associates_degree',\n",
      "       'bachelors_degree', 'graduate_degree', 'is_married', 'is_widowed',\n",
      "       'is_divorced', 'is_separated', 'is_single', 'is_female', 'is_male',\n",
      "       'income_category'],\n",
      "      dtype='object')\n",
      "(39257, 39)\n"
     ]
    }
   ],
   "source": [
    "print(ces_df_filtered.columns)\n",
    "print(ces_df_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to csv for access in later stages\n",
    "ces_df_filtered.to_csv('../../data/processed-data/ces_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Interpretation of Results:\n",
    "I ended up with a dataset with the columns as presented above. We have almost 40,000 data records to work with and 40 features which should be more than enough. We have data on total income, total expenses, online shopping expendtiture, wprking information, and demographic details. Based on that, I will be addressing the following research questions:\n",
    "\n",
    "1. Can we predict whether an individual has shopped online based on their demographic and financial information in the past year?\n",
    "2. Can we predict one's online shopping expenditure based on their annual earnings and expenses?\n",
    "3. Can we predict one's sex based on their demographics and shopping habits?\n",
    "4. Can we predict one's employment type or education level based on demographic and financial information?\n",
    "\n",
    "Please continue reading on the EDA page to further our exploration of this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
