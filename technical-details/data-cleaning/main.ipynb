{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Cleaning\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- After digesting the instructions, you can delete this cell, these are assignment instructions and do not need to be included in your final submission.  -->\n",
    "\n",
    "{{< include instructions.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code \n",
    "\n",
    "Provide the source code used for this section of the project here.\n",
    "\n",
    "If you're using a package for code organization, you can import it at this point. However, make sure that the **actual workflow steps**—including data processing, analysis, and other key tasks—are conducted and clearly demonstrated on this page. The goal is to show the technical flow of your project, highlighting how the code is executed to achieve your results.\n",
    "\n",
    "If relevant, link to additional documentation or external references that explain any complex components. This section should give readers a clear view of how the project is implemented from a technical perspective.\n",
    "\n",
    "Remember, this page is a technical narrative, NOT just a notebook with a collection of code cells, include in-line Prose, to describe what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first handled the survey data from **Pew Research Center's American Trends Panel Wave 111**. There was a decently large sample size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6034, 139)\n",
      "Index(['QKEY', 'INTERVIEW_START_W111', 'INTERVIEW_END_W111',\n",
      "       'DEVICE_TYPE_W111', 'LANG_W111', 'XTABLET_W111', 'SHOP18_W111',\n",
      "       'SHOP19_W111', 'METOO1_W111', 'METOOSUPOE_M1_W111',\n",
      "       ...\n",
      "       'F_PARTYLN_FINAL', 'F_PARTYSUM_FINAL', 'F_PARTYSUMIDEO_FINAL',\n",
      "       'F_INC_SDT1', 'F_REG', 'F_IDEO', 'F_INTFREQ', 'F_VOLSUM', 'F_INC_TIER2',\n",
      "       'WEIGHT_W111'],\n",
      "      dtype='object', length=139)\n"
     ]
    }
   ],
   "source": [
    "# Read in .sav file\n",
    "W111_df = pd.read_spss(\"../../data/raw-data/ATP_W111.sav\")\n",
    "#print(W111_df.head())\n",
    "\n",
    "#Disply data frame shape and column titles\n",
    "print(W111_df.shape)\n",
    "print(W111_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I start off by cleaning the whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and filter  \n",
    "\n",
    "# Remove whitespace from column names     \n",
    "W111_df.columns = W111_df.columns.str.strip()\n",
    "\n",
    "for col in W111_df.columns:\n",
    "\n",
    "    # Iterate through each column name and remove the suffix if present\n",
    "    if col.endswith(\"_W111\"): # Checks if column title ends with that title\n",
    "        new_col_name = col[:-5]  # Remove that part of the name\n",
    "        W111_df = W111_df.rename(columns={col: new_col_name})\n",
    "    if col.startswith(\"F_\"):\n",
    "        new_col_name = col[2:]  # Remove the first 2 characters\n",
    "        W111_df = W111_df.rename(columns={col: new_col_name})\n",
    "\n",
    "\n",
    "# Remove whitespace from each row in each column if column data type is string\n",
    "for col in W111_df.columns:\n",
    "    if W111_df[col].dtype == \"object\":\n",
    "        W111_df[col] = W111_df[col].str.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After referring to the survey's questionnaire document (included ...) to see what each feature (column) refers too, I selected the following to look into.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONLSHOP1_a    category\n",
      "ONLSHOP1_b    category\n",
      "ONLSHOP1_c    category\n",
      "SHOP4         category\n",
      "SNSUSE        category\n",
      "ONLSHOP5      category\n",
      "MARITAL       category\n",
      "USR_SELFID    category\n",
      "AGECAT        category\n",
      "GENDER        category\n",
      "EDUCCAT       category\n",
      "RACECMB       category\n",
      "INC_SDT1      category\n",
      "dtype: object\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "W111_columns_keep = [\"ONLSHOP1_a\", \"ONLSHOP1_b\", \"ONLSHOP1_c\", \"SHOP4\", \"SNSUSE\", \"ONLSHOP5\", \"MARITAL\", \"USR_SELFID\", \"AGECAT\", \n",
    "                     \"GENDER\", \"EDUCCAT\", \"RACECMB\", \"INC_SDT1\"]\n",
    "W111_df = W111_df[W111_columns_keep]\n",
    "\n",
    "# View column data types\n",
    "print(print(W111_df.dtypes) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "W111_df.rename(columns={'age': 'age_years'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONLSHOP1_a     142\n",
      "ONLSHOP1_b     142\n",
      "ONLSHOP1_c     142\n",
      "SHOP4          142\n",
      "SNSUSE         142\n",
      "ONLSHOP5      1406\n",
      "MARITAL          0\n",
      "USR_SELFID       0\n",
      "AGECAT           0\n",
      "GENDER           0\n",
      "EDUCCAT          0\n",
      "RACECMB          2\n",
      "INC_SDT1         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values per column\n",
    "null_counts = W111_df.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ONLSHOP1_a', 'ONLSHOP1_b', 'ONLSHOP1_c', 'SHOP4', 'SNSUSE', 'ONLSHOP5',\n",
      "       'MARITAL', 'USR_SELFID', 'AGECAT', 'GENDER', 'EDUCCAT', 'RACECMB',\n",
      "       'INC_SDT1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(W111_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I moved on to handling the data from the **Consumer Expenditure Survey**. For each of the datasets used from this survey, I had to go through the corresponding data dictionary (stored in an Excel) to select potentially relevant features.\n",
    "We begin with the income data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import data for income\n",
    "income_1_df = pd.read_csv(\"../../data/raw-data/itii232.csv\")\n",
    "income_2_df = pd.read_csv(\"../../data/raw-data/itii233.csv\")\n",
    "income_3_df = pd.read_csv(\"../../data/raw-data/itii234.csv\")\n",
    "income_4_df = pd.read_csv(\"../../data/raw-data/itii241.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.describe of           NEWID  REFMO  REFYR     UCC  PUBFLAG VALUE_  IMPNUM        VALUE\n",
      "0       5090604      1   2023  900030        2    NaN       1  3169.833300\n",
      "1       5090604      1   2023  900030        2    NaN       2  3169.833300\n",
      "2       5090604      1   2023  900030        2    NaN       3  3169.833300\n",
      "3       5090604      1   2023  900030        2    NaN       4  3169.833300\n",
      "4       5090604      1   2023  900030        2    NaN       5  3169.833300\n",
      "...         ...    ...    ...     ...      ...    ...     ...          ...\n",
      "330445  5366911      5   2023  980071        2    NaN       1   820.250000\n",
      "330446  5366911      5   2023  980071        2    NaN       2   250.000000\n",
      "330447  5366911      5   2023  980071        2    NaN       3   100.000000\n",
      "330448  5366911      5   2023  980071        2    NaN       4   294.666667\n",
      "330449  5366911      5   2023  980071        2    NaN       5   160.250000\n",
      "\n",
      "[330450 rows x 8 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Examine one of the dataframes\n",
    "print(income_1_df.describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I filter for the relevant columns in the income dataframes. From the data collection stage, we already know that each of dataframes has 8 columns. We want the following columns:\n",
    "- The variable \"NEWID\" represent the unique identifier for the survey participant. \n",
    "- The feature 'UCC' stands for Universal Classification Code which correpond to goods and services and other things that can be bought or sold. The values under variable \"UCC\" correspond to items that would increases or decreases to the individuals' net worth. \n",
    "- The variable \"VALUE\" indicate the absolute value of the change in net worth. The other 5 variables only represent data reelvant to the survey process so we subset the dataframes for those 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330450, 3)\n",
      "(330840, 3)\n",
      "(322320, 3)\n",
      "(325200, 3)\n"
     ]
    }
   ],
   "source": [
    "income_columns_keep = ['NEWID', 'UCC', 'VALUE']\n",
    "\n",
    "income_1_df = income_1_df[income_columns_keep]\n",
    "print(income_1_df.shape)\n",
    "\n",
    "income_2_df = income_2_df[income_columns_keep]\n",
    "print(income_2_df.shape)\n",
    "\n",
    "income_3_df = income_3_df[income_columns_keep]\n",
    "print(income_3_df.shape)\n",
    "\n",
    "income_4_df = income_4_df[income_columns_keep]\n",
    "print(income_4_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to find the unqiue \"UCC\" values to see if we have to deal with decreases in net worth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[900030, 900170, 900180, 980000, 980071, 800940, 900000, 900160, 900150, 900090, 900190, 900200, 900210, 900120, 900140]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize list that stores all unique values of 'UCC' column\n",
    "all_UCC_unique = []\n",
    "\n",
    "# Function that prints the unique values in a particular column and returns the list\n",
    "def find_unique_UCC_values(df, column_name, unique_UCC_values):\n",
    "\n",
    "  unique_values = df[column_name].unique()\n",
    "  for value in unique_values:\n",
    "    if value not in all_UCC_unique:\n",
    "        unique_UCC_values.append(value)\n",
    "        \n",
    "  \n",
    "find_unique_UCC_values(income_1_df, 'UCC', all_UCC_unique)\n",
    "find_unique_UCC_values(income_2_df, 'UCC', all_UCC_unique)\n",
    "find_unique_UCC_values(income_3_df, 'UCC', all_UCC_unique)\n",
    "find_unique_UCC_values(income_4_df, 'UCC', all_UCC_unique)\n",
    "\n",
    "print(all_UCC_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By referring to the data dictionary, I found that the \"UCC\" values are mostly associated with increases, except for 800940 which represents deductions for social security. \n",
    "There is some overlap between them. For example, 980071 represent income after taxes. Here I want to only focus on pre-tax income for simplicity's sake. Therefore we filter for the following:\n",
    "- 900030: Social Security and railroad retirement income\n",
    "- 900170: Retirement, survivors, disability income\n",
    "- 900180: Interest and dividends\n",
    "- 980000: Income before taxes\n",
    "- 800940: Deductions for Social Security\n",
    "- 900150: Food stamps\n",
    "\n",
    "The following codes correspond to income that is lumped into 980000: Income before taxes\n",
    "- 900160: Self-employment income\n",
    "- 900000: Wages and salaries \n",
    "- 900090: Supplemental security income\n",
    "- 900190: Net room/rental income\n",
    "- 900200: Royalty, estate, trust income\n",
    "- 900210: Other regular income\n",
    "- 900140: Other income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182790, 3)\n",
      "(182475, 3)\n",
      "(178470, 3)\n",
      "(179925, 3)\n"
     ]
    }
   ],
   "source": [
    "income_df_UCC_keep = [900030, 900170, 900180, 980000, 800940, 900150]\n",
    "\n",
    "negation_UCC_value = 800940\n",
    "\n",
    "# Function to filter for the 'UCC' values we want and negate if UCC = 800940\n",
    "def filter_and_negate(df, negation_ucc):\n",
    "\n",
    "  # Filter the DataFrame based on the UCC list\n",
    "  filtered_df = df[df['UCC'].isin(income_df_UCC_keep)]\n",
    "\n",
    "  # Negate the 'VALUE' column for the specific UCC\n",
    "  filtered_df.loc[filtered_df['UCC'] == negation_ucc, 'VALUE'] *= -1\n",
    "\n",
    "  return filtered_df\n",
    "\n",
    "# Apply the function to the data frames and check the shape \n",
    "income_1_df = filter_and_negate(income_1_df, negation_UCC_value)\n",
    "print(income_1_df.shape)\n",
    "\n",
    "income_2_df = filter_and_negate(income_2_df, negation_UCC_value)\n",
    "print(income_2_df.shape)\n",
    "\n",
    "income_3_df = filter_and_negate(income_3_df, negation_UCC_value)\n",
    "print(income_3_df.shape)\n",
    "\n",
    "income_4_df = filter_and_negate(income_4_df, negation_UCC_value)\n",
    "print(income_4_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cleaned income dataframes, we aggregate each to find the total income per year per individual and merge all the dataframes to get the master income dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18829, 2)\n"
     ]
    }
   ],
   "source": [
    "# Function sums income sources based on participant ID \n",
    "def calculate_total_income(df):\n",
    "\n",
    "#use reset_index to make a hierarchical index a regular column\n",
    "  total_income_df = df.groupby('NEWID')['VALUE'].sum().reset_index() \n",
    "  \n",
    "  # Rename columns in place\n",
    "  total_income_df.columns = ['id', 'total_income']\n",
    "  return total_income_df\n",
    "\n",
    "\n",
    "# Calculate total income for each DataFrame\n",
    "total_income_df1 = calculate_total_income(income_1_df)\n",
    "total_income_df2 = calculate_total_income(income_2_df)\n",
    "total_income_df3 = calculate_total_income(income_3_df)\n",
    "total_income_df4 = calculate_total_income(income_4_df)\n",
    "\n",
    "# Concatenate dataframes to get total income per survey participant\n",
    "total_income_df = pd.concat([total_income_df1, total_income_df2, total_income_df3, total_income_df4], axis = 0)\n",
    "\n",
    "#DF of income over a year\n",
    "print(total_income_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of            id  total_income\n",
      "0     5090604   101692.5000\n",
      "1     5090624    34467.5010\n",
      "2     5090634   155839.9995\n",
      "3     5090664    72695.0001\n",
      "4     5090674    43196.2500\n",
      "...       ...           ...\n",
      "4675  5607961   130770.0000\n",
      "4676  5607981   364462.6290\n",
      "4677  5608001    84775.0005\n",
      "4678  5608051   486724.8864\n",
      "4679  5608061    65240.0010\n",
      "\n",
      "[18829 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(total_income_df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we handle the expenditures data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import data for expenses\n",
    "expense_1_df = pd.read_csv(\"../../data/raw-data/mtbi232.csv\")\n",
    "expense_2_df = pd.read_csv(\"../../data/raw-data/mtbi233.csv\")\n",
    "expense_3_df = pd.read_csv(\"../../data/raw-data/mtbi234.csv\")\n",
    "expense_4_df = pd.read_csv(\"../../data/raw-data/mtbi241.csv\")\n",
    "\n",
    "# Subset expense Data Frames for the relevant columns\n",
    "expense_columns_keep = ['NEWID', 'SEQNO', 'UCC', 'COST']\n",
    "expense_1_df = expense_1_df[expense_columns_keep]\n",
    "expense_2_df = expense_2_df[expense_columns_keep]\n",
    "expense_3_df = expense_3_df[expense_columns_keep]\n",
    "expense_4_df = expense_4_df[expense_columns_keep]\n",
    "\n",
    "expense_df = pd.concat([expense_1_df, expense_2_df, expense_3_df, expense_4_df], axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By consulting the data dictionary, I located the specific files that listed the online purchases of tangible goods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/dt8s0j7j0glg1jfrlpndm0lw0000gn/T/ipykernel_1037/2021987445.py:1: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  specific_expense_df1 = pd.read_csv(\"../../data/raw-data/apb23.csv\")\n"
     ]
    }
   ],
   "source": [
    "specific_expense_df1 = pd.read_csv(\"../../data/raw-data/apb23.csv\")\n",
    "specific_expense_df2 = pd.read_csv(\"../../data/raw-data/eqb23.csv\")\n",
    "specific_expense_df3 = pd.read_csv(\"../../data/raw-data/mis23.csv\")\n",
    "specific_expense_df4 = pd.read_csv(\"../../data/raw-data/ovb23.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['QYEAR', 'NEWID', 'SEQNO', 'ALCNO', 'REC_ORIG', 'MINAPPLY', 'MINA_PLY',\n",
      "       'GFTCMIN', 'GFTCMIN_', 'MIN_MO', 'MIN_MO_', 'MINPURX', 'MINPURX_',\n",
      "       'MINRENTX', 'MINR_NTX', 'MNAPPL1', 'MNAPPL1_', 'MNAPPL2', 'MNAPPL2_',\n",
      "       'MNAPPL3', 'MNAPPL3_', 'MNAPPL4', 'MNAPPL4_', 'MNAPPL5', 'MNAPPL5_',\n",
      "       'MNAPPL6', 'MNAPPL6_', 'MNAPPL7', 'MNAPPL7_', 'MNAPPL8', 'MNAPPL8_',\n",
      "       'MNAPPL9', 'MNAPPL9_', 'INSTLSCR', 'INST_SCR', 'INSTLLEX', 'INST_LEX',\n",
      "       'APBPURCH'],\n",
      "      dtype='object')\n",
      "Index(['QYEAR', 'NEWID', 'SEQNO', 'ALCNO', 'REC_ORIG', 'APPRPRYB', 'APPR_RYB',\n",
      "       'SRVCMOB', 'SRVCMOB_', 'REPAIRX', 'REPAIRX_', 'APPRPB1', 'APPRPB1_',\n",
      "       'APPRPB2', 'APPRPB2_', 'APPRPB3', 'APPRPB3_', 'APPRPB4', 'APPRPB4_',\n",
      "       'APPRPB5', 'APPRPB5_', 'APPRPB6', 'APPRPB6_', 'APPRPB7', 'APPRPB7_',\n",
      "       'APPRPB8', 'APPRPB8_', 'APPRPB9', 'EQBPURCH', 'APPRPB9_'],\n",
      "      dtype='object')\n",
      "Index(['QYEAR', 'NEWID', 'SEQNO', 'ALCNO', 'REC_ORIG', 'MISCCODE', 'MISC_ODE',\n",
      "       'MISCMO', 'MISCMO_', 'MISCGFTC', 'MISC_FTC', 'MISCEXPX', 'MISC_XPX',\n",
      "       'MISCDE1', 'MISCDE1_', 'MISCDE2', 'MISCDE2_', 'MISCDE3', 'MISCDE3_',\n",
      "       'MISCDE4', 'MISCDE4_', 'MISCDE5', 'MISCDE5_', 'MISCDE6', 'MISCDE6_',\n",
      "       'MISCDE7', 'MISCDE7_', 'MISCDE8', 'MISCDE8_', 'MISCDE9', 'MISPURCH',\n",
      "       'MISCDE9_'],\n",
      "      dtype='object')\n",
      "Index(['QYEAR', 'NEWID', 'SEQNO', 'ALCNO', 'REC_ORIG', 'VEHICYB', 'VEHICYB_',\n",
      "       'VEHICYR', 'VEHICYR_', 'VEHBSNZ', 'VEHBSNZ_', 'VEHNEWU', 'VEHNEWU_',\n",
      "       'VEHGFTC', 'VEHGFTC_', 'VEHPURMO', 'VEHP_RMO', 'VEHPURYR', 'VEHP_RYR',\n",
      "       'VFINSTAT', 'VFIN_TAT', 'TRADEX', 'TRADEX_', 'NETPURX', 'NETPURX_',\n",
      "       'DNPAYMTX', 'DNPA_MTX', 'PRINCIPX', 'PRIN_IPX', 'VEHQPMT', 'VEHQPMT_',\n",
      "       'PMT1MO', 'PMT1MO_', 'PMT1YR', 'PMT1YR_', 'PAYMENTX', 'PAYM_NTX',\n",
      "       'QINTRSTZ', 'QINT_STZ', 'QLOANM1Q', 'QLOA_M1Q', 'QBALNM1X', 'QBAL_M1X',\n",
      "       'QVINTM1X', 'QVIN_M1X', 'QVPRIM1X', 'QVPR_M1X', 'QLOANM2Q', 'QLOA_M2Q',\n",
      "       'QBALNM2X', 'QBAL_M2X', 'QVINTM2X', 'QVIN_M2X', 'QVPRIM2X', 'QVPR_M2X',\n",
      "       'QLOANM3Q', 'QLOA_M3Q', 'QBALNM3X', 'QBAL_M3X', 'QVINTM3X', 'QVIN_M3X',\n",
      "       'QVPRIM3X', 'QVPR_M3X', 'QTRADEX', 'QTRADEX_', 'QREIMBRZ', 'QREI_BRZ',\n",
      "       'QADITR1X', 'QADI_R1X', 'QADITR2X', 'QADI_R2X', 'QADITR3X', 'QADI_R3X',\n",
      "       'QDNPYMTX', 'QDNP_MTX', 'VEHEQTLN', 'VEHE_TLN', 'VEHICIB', 'VEHICIB_',\n",
      "       'MAKE', 'MAKE_', 'OVBPURCH', 'VFINANCE', 'FUELTYPE', 'FUEL_YPE',\n",
      "       'VPURINDV', 'VPUR_NDV', 'VINTRATE', 'VINT_ATE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(specific_expense_df1.columns)\n",
    "print(specific_expense_df2.columns)\n",
    "print(specific_expense_df3.columns)\n",
    "print(specific_expense_df4.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the specific purchase Data Frames for the relevant columns so we get four dataframes of online expenses.\n",
    "- 'SEQNO' is the identifier variable for the purchases and can be used to merge with expense data frames. \n",
    "- 'APBPURCH' tells us if this item was purchased online or in-person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "apb_columns_keep = ['NEWID', 'SEQNO', 'APBPURCH']\n",
    "online_expense_df1 = specific_expense_df1[apb_columns_keep]\n",
    "online_expense_df1_subset = online_expense_df1.loc[online_expense_df1['APBPURCH'] == 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'EQBPURCH' tells us if this item was bought online or in-person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqb_columns_keep = ['NEWID', 'SEQNO', 'EQBPURCH']\n",
    "online_expense_df2 = specific_expense_df2[eqb_columns_keep]\n",
    "online_expense_df2_subset = online_expense_df2.loc[online_expense_df2['EQBPURCH'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'MISPURCH' tells us if this item was bought online or in-person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_columns_keep = ['NEWID', 'SEQNO', 'MISPURCH']\n",
    "online_expense_df3 = specific_expense_df3[mis_columns_keep]\n",
    "online_expense_df3_subset = online_expense_df3.loc[online_expense_df3['MISPURCH'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'OVBPURCH' tells us if this item was bought online or in-person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovb_columns_keep = ['NEWID', 'SEQNO', 'OVBPURCH']\n",
    "online_expense_df4 = specific_expense_df4[ovb_columns_keep]\n",
    "online_expense_df4_subset = online_expense_df4.loc[online_expense_df4['OVBPURCH'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the mini-dataframes to get the the online_expense_df which we will merge with the expense_df to see the dollar amount of the online purchase. Then, we aggregate based upon 'NEWID' primary key to calculate the total expense per person, total online expense and the online spending percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online Expense Columns:  Index(['NEWID', 'SEQNO', 'Is_Online'], dtype='object')\n",
      "Merged Expense Columns:  Index(['NEWID', 'SEQNO', 'UCC', 'COST', 'Is_Online'], dtype='object')\n",
      "Total Expense Columns:  Index(['id', 'Total_Expense', 'Online_Expense', 'Online_Percentage'], dtype='object')\n",
      "(18871, 4)\n"
     ]
    }
   ],
   "source": [
    "#Concatenate all online expense df\n",
    "online_expense_df = pd.concat([online_expense_df1_subset, online_expense_df2_subset, online_expense_df3_subset, online_expense_df4_subset], axis = 0)\n",
    "\n",
    "# Filter for relevant columns\n",
    "online_expense_df = online_expense_df[['NEWID', 'SEQNO']]\n",
    "\n",
    "# Add a column with imputed constant values of 1 for later merging so we know the expenses are online \n",
    "online_expense_df['Is_Online'] = 1\n",
    "print(\"Online Expense Columns: \",online_expense_df.columns)\n",
    "\n",
    "# Merge the two DataFrames based on 'NEWID' and 'SEQNO'\n",
    "merged_expense_df = expense_df.merge(online_expense_df, on=['NEWID', 'SEQNO'], how='left')\n",
    "print(\"Merged Expense Columns: \", merged_expense_df.columns)\n",
    "\n",
    "\n",
    "# Calculate total expenses, online expenses, and online percentage for each individual\n",
    "# Group by 'NEWID' column, and perform 3 aggregate functions\n",
    "total_expense_df = merged_expense_df.groupby('NEWID').agg(\n",
    "\n",
    "    id=('NEWID', 'first'),\n",
    "\n",
    "    #create a new column, and calculates the sum of 'COST'\n",
    "    Total_Expense=('COST', 'sum'), \n",
    "\n",
    "    #filter COST column to include only rows where Is_Online is 1 ( online purchases)\n",
    "    Online_Expense=('COST', lambda x: x[merged_expense_df['Is_Online'] == 1].sum()), \n",
    "\n",
    "    #filter COST column for online purchases and claculates that as a % of total\n",
    "    Online_Percentage=('COST', lambda x: x[merged_expense_df['Is_Online'] == 1].sum() / x.sum() * 100)\n",
    ")\n",
    "\n",
    "print(\"Total Expense Columns: \", total_expense_df.columns)\n",
    "print(total_expense_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to handling characteristics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data for individual characteristics\n",
    "characteristics_df_1 = pd.read_csv(\"../../data/raw-data/memi232.csv\")\n",
    "characteristics_df_2 = pd.read_csv(\"../../data/raw-data/memi233.csv\")\n",
    "characteristics_df_3 = pd.read_csv(\"../../data/raw-data/memi234.csv\")\n",
    "characteristics_df_4 = pd.read_csv(\"../../data/raw-data/memi241.csv\")\n",
    "\n",
    "# Filter for  relevant columns\n",
    "characteristics_columns_keep = [\"AGE\", \"ARM_FORC\", \"EARNTYPE\", \"EDUCA\", \"INCWEEKQ\", \n",
    "                                \"INDRETX\", \"JSSDEDX\", \"JSSDEDXM\", \"MARITAL\", \"MEMBRACE\", \"RC_ASIAN\", \"RC_BLACK\", \"RC_DK\", \"RC_NATAM\",\n",
    "                                \"RC_OTHER\", \"RC_PACIL\", \"RC_WHITE\", \"SEX\", \"SOCRRX\"]\n",
    "\n",
    "characteristics_df_1 = characteristics_df_1[characteristics_columns_keep]\n",
    "characteristics_df_2 = characteristics_df_2[characteristics_columns_keep]\n",
    "characteristics_df_3 = characteristics_df_3[characteristics_columns_keep]\n",
    "characteristics_df_4 = characteristics_df_4[characteristics_columns_keep]\n",
    "\n",
    "# Combine all of them\n",
    "characteristics_df = pd.concat([characteristics_df_1, characteristics_df_2, characteristics_df_3, characteristics_df_4], axis = 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "characteristics_df.rename(columns={\"AGE\": \"age\", \"ARM_FORC\": \"is_military\",  \"EARNTYPE\": \"earning_type\", \"EDUCA\": \"highest_ed_completed\", \n",
    "                                   \"INCWEEKQ\": \"num_weeks_worked_in_last_yr\", \"INDRETX\": \"deposited_money_in_retirement_this_yr\", \n",
    "                                   \"JSSDEDX\": \"income_into_SS_this_yr\", \"JSSDEDXM\": \"SS_payments_received_this_yr\", \"MARITAL\": \"marital_status\", \n",
    "                                   \"MEMBRACE\": \"race\", \"RC_ASIAN\": \"is_asian\", \"RC_BLACK\": \"is_black\", \"RC_DK\": \"race_unknown\", \n",
    "                                   \"RC_NATAM\": \"is_native_american\", \"RC_OTHER\": \"is_other_race\", \"RC_PACIL\": \"is_pacific_islander\", \n",
    "                                   \"RC_WHITE\": \"is_white\", \"SEX\": \"sex\", \"SOCRRX\": \"SS_and_railroad_retirement_income_received\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some data cleaning for the categorical variables and one-hot encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning for one-hot encoding is_military table\n",
    "characteristics_df[\"is_military\"] = characteristics_df['is_military'].replace(2, 0)\n",
    "\n",
    "\n",
    "# One-hot encode the 'earning_type' column\n",
    "characteristics_df = pd.get_dummies(characteristics_df, columns=['earning_type'], prefix='', prefix_sep='', dtype = int)\n",
    "\n",
    "# Rename the earning_type columns\n",
    "characteristics_df.rename(columns={'1.0': 'full_time_1_yr', \n",
    "                   '2.0': 'part_time_1_yr', \n",
    "                   '3.0': 'full_time_part_yr', \n",
    "                   '4.0': 'part_time_part_yr'}, inplace=True)\n",
    "\n",
    "# One-hot encode the 'highest_ed_completed' column\n",
    "characteristics_df = pd.get_dummies(characteristics_df, columns=['highest_ed_completed'], prefix='', prefix_sep='', dtype = int)\n",
    "\n",
    "# Rename the highest_ed_completed columns\n",
    "characteristics_df.rename(columns={'1.0': 'no_school_completed', \n",
    "                   '2.0': 'grades_1-8_completed', \n",
    "                   '3.0': 'high_school_no_degree', \n",
    "                   '4.0': 'high_school_grad',\n",
    "                   '5.0': 'some_college_no_degree',\n",
    "                   '6.0': 'associates_degree',\n",
    "                   '7.0': 'bachelors_degree',\n",
    "                   '8.0': 'graduate_degree'}, inplace=True)\n",
    "\n",
    "# One-hot encode the 'marital_status' column\n",
    "characteristics_df = pd.get_dummies(characteristics_df, columns=['marital_status'], prefix='', prefix_sep='', dtype = int)\n",
    "\n",
    "# Rename the marital_status columns\n",
    "characteristics_df.rename(columns={'1': 'is_married', \n",
    "                   '2': 'is_widowed', \n",
    "                   '3': 'is_divorced', \n",
    "                   '4': 'is_separated',\n",
    "                   '5': 'is_single'}, inplace=True)\n",
    "\n",
    "# One-hot encode the 'sex' column\n",
    "characteristics_df = pd.get_dummies(characteristics_df, columns=['sex'], prefix='', prefix_sep='', dtype = int)\n",
    "\n",
    "# Rename the sex columns\n",
    "characteristics_df.rename(columns={'1': 'is_male', \n",
    "                   '2': 'is_female'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race\n",
      "1    34631\n",
      "2     4560\n",
      "4     3405\n",
      "6     1040\n",
      "5      259\n",
      "3      235\n",
      "Name: count, dtype: int64\n",
      "is_asian\n",
      "4.0    3714\n",
      "Name: count, dtype: int64\n",
      "is_black\n",
      "2.0    4784\n",
      "Name: count, dtype: int64\n",
      "race_unknown\n",
      "7.0    297\n",
      "Name: count, dtype: int64\n",
      "is_native_american\n",
      "3.0    432\n",
      "Name: count, dtype: int64\n",
      "is_other_race\n",
      "6.0    2595\n",
      "Name: count, dtype: int64\n",
      "is_pacific_islander\n",
      "5.0    415\n",
      "Name: count, dtype: int64\n",
      "is_white\n",
      "1.0    32913\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# View values per column about race\n",
    "# Legend: White - 1, Black - 2, Native American - 3, Asian - 4, Pacific Islander - 5, Other - 6, Unknown - 7\n",
    "race_columns = [\"is_asian\", \"is_black\", \"race_unknown\", \"is_native_american\", \"is_other_race\", \"is_pacific_islander\", \"is_white\"]\n",
    "print(characteristics_df[\"race\"].value_counts())\n",
    "for race in race_columns:\n",
    "    print(characteristics_df[race].value_counts())\n",
    "\n",
    "# One-Hot Encode Race Columns\n",
    "# Replace numerical values for one-hot-encoding\n",
    "characteristics_df[\"is_asian\"] = characteristics_df['is_asian'].replace(4, 1)\n",
    "characteristics_df[\"is_black\"] = characteristics_df['is_black'].replace(2, 1)\n",
    "characteristics_df[\"race_unknown\"] = characteristics_df['race_unknown'].replace(7, 1)\n",
    "characteristics_df[\"is_native_american\"] = characteristics_df['is_native_american'].replace(3, 1)\n",
    "characteristics_df[\"is_other_race\"] = characteristics_df['is_other_race'].replace(6, 1)\n",
    "characteristics_df[\"is_pacific_islander\"] = characteristics_df['is_pacific_islander'].replace(5, 1)\n",
    "\n",
    "# Replace NaN values with 0\n",
    "characteristics_df[race_columns] = characteristics_df[race_columns].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'is_military', 'num_weeks_worked_in_last_yr',\n",
      "       'deposited_money_in_retirement_this_yr', 'income_into_SS_this_yr',\n",
      "       'SS_payments_received_this_yr', 'race', 'is_asian', 'is_black',\n",
      "       'race_unknown', 'is_native_american', 'is_other_race',\n",
      "       'is_pacific_islander', 'is_white',\n",
      "       'SS_and_railroad_retirement_income_received', 'full_time_1_yr',\n",
      "       'part_time_1_yr', 'full_time_part_yr', 'part_time_part_yr',\n",
      "       'no_school_completed', 'grades_1-8_completed', 'high_school_no_degree',\n",
      "       'high_school_grad', 'some_college_no_degree', 'associates_degree',\n",
      "       'bachelors_degree', 'graduate_degree', 'is_married', 'is_widowed',\n",
      "       'is_divorced', 'is_separated', 'is_single', 'is_male', 'is_female'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(characteristics_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
