{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Cleaning\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- After digesting the instructions, you can delete this cell, these are assignment instructions and do not need to be included in your final submission.  -->\n",
    "\n",
    "{{< include instructions.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code \n",
    "\n",
    "Provide the source code used for this section of the project here.\n",
    "\n",
    "If you're using a package for code organization, you can import it at this point. However, make sure that the **actual workflow steps**—including data processing, analysis, and other key tasks—are conducted and clearly demonstrated on this page. The goal is to show the technical flow of your project, highlighting how the code is executed to achieve your results.\n",
    "\n",
    "If relevant, link to additional documentation or external references that explain any complex components. This section should give readers a clear view of how the project is implemented from a technical perspective.\n",
    "\n",
    "Remember, this page is a technical narrative, NOT just a notebook with a collection of code cells, include in-line Prose, to describe what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first handled the survey data from **Pew Research Center's American Trends Panel Wave 111**. There was a decently large sample size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6034, 139)\n",
      "Index(['QKEY', 'INTERVIEW_START_W111', 'INTERVIEW_END_W111',\n",
      "       'DEVICE_TYPE_W111', 'LANG_W111', 'XTABLET_W111', 'SHOP18_W111',\n",
      "       'SHOP19_W111', 'METOO1_W111', 'METOOSUPOE_M1_W111',\n",
      "       ...\n",
      "       'F_PARTYLN_FINAL', 'F_PARTYSUM_FINAL', 'F_PARTYSUMIDEO_FINAL',\n",
      "       'F_INC_SDT1', 'F_REG', 'F_IDEO', 'F_INTFREQ', 'F_VOLSUM', 'F_INC_TIER2',\n",
      "       'WEIGHT_W111'],\n",
      "      dtype='object', length=139)\n"
     ]
    }
   ],
   "source": [
    "# Read in .sav file\n",
    "W111_df = pd.read_spss(\"../../data/raw-data/ATP_W111.sav\")\n",
    "#print(W111_df.head())\n",
    "\n",
    "#Disply data frame shape and column titles\n",
    "print(W111_df.shape)\n",
    "print(W111_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I start off by cleaning the whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and filter  \n",
    "\n",
    "# Remove whitespace from column names     \n",
    "W111_df.columns = W111_df.columns.str.strip()\n",
    "\n",
    "for col in W111_df.columns:\n",
    "\n",
    "    # Iterate through each column name and remove the suffix if present\n",
    "    if col.endswith(\"_W111\"): # Checks if column title ends with that title\n",
    "        new_col_name = col[:-5]  # Remove that part of the name\n",
    "        W111_df = W111_df.rename(columns={col: new_col_name})\n",
    "    if col.startswith(\"F_\"):\n",
    "        new_col_name = col[2:]  # Remove the first 2 characters\n",
    "        W111_df = W111_df.rename(columns={col: new_col_name})\n",
    "\n",
    "\n",
    "# Remove whitespace from each row in each column if column data type is string\n",
    "for col in W111_df.columns:\n",
    "    if W111_df[col].dtype == \"object\":\n",
    "        W111_df[col] = W111_df[col].str.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After referring to the survey's questionnaire document (included ...) to see what each feature (column) refers too, I selected the following to look into.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONLSHOP1_a    category\n",
      "ONLSHOP1_b    category\n",
      "ONLSHOP1_c    category\n",
      "SHOP4         category\n",
      "SNSUSE        category\n",
      "ONLSHOP5      category\n",
      "MARITAL       category\n",
      "USR_SELFID    category\n",
      "AGECAT        category\n",
      "GENDER        category\n",
      "EDUCCAT       category\n",
      "RACECMB       category\n",
      "INC_SDT1      category\n",
      "dtype: object\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "W111_columns_keep = [\"ONLSHOP1_a\", \"ONLSHOP1_b\", \"ONLSHOP1_c\", \"SHOP4\", \"SNSUSE\", \"ONLSHOP5\", \"MARITAL\", \"USR_SELFID\", \"AGECAT\", \n",
    "                     \"GENDER\", \"EDUCCAT\", \"RACECMB\", \"INC_SDT1\"]\n",
    "W111_df = W111_df[W111_columns_keep]\n",
    "\n",
    "# View column data types\n",
    "print(print(W111_df.dtypes) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "W111_df.rename(columns={'age': 'age_years'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONLSHOP1_a     142\n",
      "ONLSHOP1_b     142\n",
      "ONLSHOP1_c     142\n",
      "SHOP4          142\n",
      "SNSUSE         142\n",
      "ONLSHOP5      1406\n",
      "MARITAL          0\n",
      "USR_SELFID       0\n",
      "AGECAT           0\n",
      "GENDER           0\n",
      "EDUCCAT          0\n",
      "RACECMB          2\n",
      "INC_SDT1         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values per column\n",
    "null_counts = W111_df.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ONLSHOP1_a', 'ONLSHOP1_b', 'ONLSHOP1_c', 'SHOP4', 'SNSUSE', 'ONLSHOP5',\n",
      "       'MARITAL', 'USR_SELFID', 'AGECAT', 'GENDER', 'EDUCCAT', 'RACECMB',\n",
      "       'INC_SDT1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(W111_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I moved on to handling the data from the Consumer Expenditure Survey. We begin with the income data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import data for income\n",
    "income_1_df = pd.read_csv(\"../../data/raw-data/itii232.csv\")\n",
    "income_2_df = pd.read_csv(\"../../data/raw-data/itii233.csv\")\n",
    "income_3_df = pd.read_csv(\"../../data/raw-data/itii234.csv\")\n",
    "income_4_df = pd.read_csv(\"../../data/raw-data/itii241.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.describe of           NEWID  REFMO  REFYR     UCC  PUBFLAG VALUE_  IMPNUM        VALUE\n",
      "0       5090604      1   2023  900030        2    NaN       1  3169.833300\n",
      "1       5090604      1   2023  900030        2    NaN       2  3169.833300\n",
      "2       5090604      1   2023  900030        2    NaN       3  3169.833300\n",
      "3       5090604      1   2023  900030        2    NaN       4  3169.833300\n",
      "4       5090604      1   2023  900030        2    NaN       5  3169.833300\n",
      "...         ...    ...    ...     ...      ...    ...     ...          ...\n",
      "330445  5366911      5   2023  980071        2    NaN       1   820.250000\n",
      "330446  5366911      5   2023  980071        2    NaN       2   250.000000\n",
      "330447  5366911      5   2023  980071        2    NaN       3   100.000000\n",
      "330448  5366911      5   2023  980071        2    NaN       4   294.666667\n",
      "330449  5366911      5   2023  980071        2    NaN       5   160.250000\n",
      "\n",
      "[330450 rows x 8 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(income_1_df.describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I filter for the relevant columns in the income dataframes. From the data collection stage, we already know that each of dataframes has 8 columns. We want the following columns:\n",
    "- The variable \"NEWID\" represent the unique identifier for the survey participant. \n",
    "- The feature 'UCC' stands for Universal Classification Code which correpond to goods and services and other things that can be bought or sold. The values under variable \"UCC\" correspond to items that would increases or decreases to the individuals' net worth. \n",
    "- The variable \"VALUE\" indicate the absolute value of the change in net worth. The other 5 variables only represent data reelvant to the survey process so we subset the dataframes for those 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330450, 3)\n",
      "(330840, 3)\n",
      "(322320, 3)\n",
      "(325200, 3)\n"
     ]
    }
   ],
   "source": [
    "income_columns_keep = ['NEWID', 'UCC', 'VALUE']\n",
    "\n",
    "income_1_df = income_1_df[income_columns_keep]\n",
    "print(income_1_df.shape)\n",
    "\n",
    "income_2_df = income_2_df[income_columns_keep]\n",
    "print(income_2_df.shape)\n",
    "\n",
    "income_3_df = income_3_df[income_columns_keep]\n",
    "print(income_3_df.shape)\n",
    "\n",
    "income_4_df = income_4_df[income_columns_keep]\n",
    "print(income_4_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to find the unqiue \"UCC\" values to see if we have to deal with decreases in net worth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[900030, 900170, 900180, 980000, 980071, 800940, 900000, 900160, 900150, 900090, 900190, 900200, 900210, 900120, 900140]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize list that stores all unique values of 'UCC' column\n",
    "all_UCC_unique = []\n",
    "\n",
    "# Function that prints the unique values in a particular column and returns the list\n",
    "def find_unique_UCC_values(df, column_name, unique_UCC_values):\n",
    "\n",
    "  unique_values = df[column_name].unique()\n",
    "  for value in unique_values:\n",
    "    if value not in all_UCC_unique:\n",
    "        unique_UCC_values.append(value)\n",
    "        \n",
    "  \n",
    "find_unique_UCC_values(income_1_df, 'UCC', all_UCC_unique)\n",
    "find_unique_UCC_values(income_2_df, 'UCC', all_UCC_unique)\n",
    "find_unique_UCC_values(income_3_df, 'UCC', all_UCC_unique)\n",
    "find_unique_UCC_values(income_4_df, 'UCC', all_UCC_unique)\n",
    "\n",
    "print(all_UCC_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By referring to the data dictionary, I found that the \"UCC\" values are mostly associated with increases, except for 800940 which represents deductions for social security. \n",
    "There is some overlap between them. For example, 980071 represent income after taxes. Here I want to only focus on pre-tax income for simplicity's sake. Therefore we filter for the following:\n",
    "- 900030: Social Security and railroad retirement income\n",
    "- 900170: Retirement, survivors, disability income\n",
    "- 900180: Interest and dividends\n",
    "- 980000: Income before taxes\n",
    "- 800940: Deductions for Social Security\n",
    "- 900150: Food stamps\n",
    "\n",
    "The following codes correspond to income that is lumped into 980000: Income before taxes\n",
    "- 900160: Self-employment income\n",
    "- 900000: Wages and salaries \n",
    "- 900090: Supplemental security income\n",
    "- 900190: Net room/rental income\n",
    "- 900200: Royalty, estate, trust income\n",
    "- 900210: Other regular income\n",
    "- 900140: Other income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182790, 3)\n",
      "(182475, 3)\n",
      "(178470, 3)\n",
      "(179925, 3)\n"
     ]
    }
   ],
   "source": [
    "income_df_UCC_keep = [900030, 900170, 900180, 980000, 800940, 900150]\n",
    "\n",
    "negation_UCC_value = 800940\n",
    "\n",
    "# Function to filter for the 'UCC' values we want and negate if UCC = 800940\n",
    "def filter_and_negate(df, negation_ucc):\n",
    "\n",
    "  # Filter the DataFrame based on the UCC list\n",
    "  filtered_df = df[df['UCC'].isin(income_df_UCC_keep)]\n",
    "\n",
    "  # Negate the 'VALUE' column for the specific UCC\n",
    "  filtered_df.loc[filtered_df['UCC'] == negation_ucc, 'VALUE'] *= -1\n",
    "\n",
    "  return filtered_df\n",
    "\n",
    "# Apply the function to the data frames and check the shape \n",
    "income_1_df = filter_and_negate(income_1_df, negation_UCC_value)\n",
    "print(income_1_df.shape)\n",
    "\n",
    "income_2_df = filter_and_negate(income_2_df, negation_UCC_value)\n",
    "print(income_2_df.shape)\n",
    "\n",
    "income_3_df = filter_and_negate(income_3_df, negation_UCC_value)\n",
    "print(income_3_df.shape)\n",
    "\n",
    "income_4_df = filter_and_negate(income_4_df, negation_UCC_value)\n",
    "print(income_4_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cleaned income dataframes, we aggregate each to find the total income per year per individual and merge all the dataframes to get the master income dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18829, 2)\n"
     ]
    }
   ],
   "source": [
    "# Function sums income sources based on participant ID \n",
    "def calculate_total_income(df):\n",
    "\n",
    "#use reset_index to make a hierarchical index a regular column\n",
    "  total_income_df = df.groupby('NEWID')['VALUE'].sum().reset_index() \n",
    "  \n",
    "  # Rename columns in place\n",
    "  total_income_df.columns = ['id', 'total_income']\n",
    "  return total_income_df\n",
    "\n",
    "\n",
    "# Calculate total income for each DataFrame\n",
    "total_income_df1 = calculate_total_income(income_1_df)\n",
    "total_income_df2 = calculate_total_income(income_2_df)\n",
    "total_income_df3 = calculate_total_income(income_3_df)\n",
    "total_income_df4 = calculate_total_income(income_4_df)\n",
    "\n",
    "# Concatenate dataframes to get total income per survey participant\n",
    "total_income_df = pd.concat([total_income_df1, total_income_df2, total_income_df3, total_income_df4], axis = 0)\n",
    "\n",
    "#DF of income over a year\n",
    "print(total_income_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of            id  total_income\n",
      "0     5090604   101692.5000\n",
      "1     5090624    34467.5010\n",
      "2     5090634   155839.9995\n",
      "3     5090664    72695.0001\n",
      "4     5090674    43196.2500\n",
      "...       ...           ...\n",
      "4675  5607961   130770.0000\n",
      "4676  5607981   364462.6290\n",
      "4677  5608001    84775.0005\n",
      "4678  5608051   486724.8864\n",
      "4679  5608061    65240.0010\n",
      "\n",
      "[18829 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(total_income_df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we handle the expenditures data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import data for expenses\n",
    "expense_1_df = pd.read_csv(\"../../data/raw-data/mtbi232.csv\")\n",
    "expense_2_df = pd.read_csv(\"../../data/raw-data/mtbi233.csv\")\n",
    "expense_3_df = pd.read_csv(\"../../data/raw-data/mtbi234.csv\")\n",
    "expense_4_df = pd.read_csv(\"../../data/raw-data/mtbi241.csv\")\n",
    "\n",
    "# Subset expense Data Frames for the relevant columns\n",
    "expense_columns_keep = ['NEWID', 'SEQNO', 'UCC', 'COST']\n",
    "expense_1_df = expense_1_df[expense_columns_keep]\n",
    "expense_2_df = expense_2_df[expense_columns_keep]\n",
    "expense_3_df = expense_3_df[expense_columns_keep]\n",
    "expense_4_df = expense_4_df[expense_columns_keep]\n",
    "\n",
    "expense_df = pd.concat([expense_1_df, expense_2_df, expense_3_df, expense_4_df], axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By consulting the data dictionary, I located the specific files that listed the online purchases of tangible goods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/dt8s0j7j0glg1jfrlpndm0lw0000gn/T/ipykernel_63251/2021987445.py:1: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  specific_expense_df1 = pd.read_csv(\"../../data/raw-data/apb23.csv\")\n"
     ]
    }
   ],
   "source": [
    "specific_expense_df1 = pd.read_csv(\"../../data/raw-data/apb23.csv\")\n",
    "specific_expense_df2 = pd.read_csv(\"../../data/raw-data/eqb23.csv\")\n",
    "specific_expense_df3 = pd.read_csv(\"../../data/raw-data/mis23.csv\")\n",
    "specific_expense_df4 = pd.read_csv(\"../../data/raw-data/ovb23.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['QYEAR', 'NEWID', 'SEQNO', 'ALCNO', 'REC_ORIG', 'MINAPPLY', 'MINA_PLY',\n",
      "       'GFTCMIN', 'GFTCMIN_', 'MIN_MO', 'MIN_MO_', 'MINPURX', 'MINPURX_',\n",
      "       'MINRENTX', 'MINR_NTX', 'MNAPPL1', 'MNAPPL1_', 'MNAPPL2', 'MNAPPL2_',\n",
      "       'MNAPPL3', 'MNAPPL3_', 'MNAPPL4', 'MNAPPL4_', 'MNAPPL5', 'MNAPPL5_',\n",
      "       'MNAPPL6', 'MNAPPL6_', 'MNAPPL7', 'MNAPPL7_', 'MNAPPL8', 'MNAPPL8_',\n",
      "       'MNAPPL9', 'MNAPPL9_', 'INSTLSCR', 'INST_SCR', 'INSTLLEX', 'INST_LEX',\n",
      "       'APBPURCH'],\n",
      "      dtype='object')\n",
      "Index(['QYEAR', 'NEWID', 'SEQNO', 'ALCNO', 'REC_ORIG', 'APPRPRYB', 'APPR_RYB',\n",
      "       'SRVCMOB', 'SRVCMOB_', 'REPAIRX', 'REPAIRX_', 'APPRPB1', 'APPRPB1_',\n",
      "       'APPRPB2', 'APPRPB2_', 'APPRPB3', 'APPRPB3_', 'APPRPB4', 'APPRPB4_',\n",
      "       'APPRPB5', 'APPRPB5_', 'APPRPB6', 'APPRPB6_', 'APPRPB7', 'APPRPB7_',\n",
      "       'APPRPB8', 'APPRPB8_', 'APPRPB9', 'EQBPURCH', 'APPRPB9_'],\n",
      "      dtype='object')\n",
      "Index(['QYEAR', 'NEWID', 'SEQNO', 'ALCNO', 'REC_ORIG', 'MISCCODE', 'MISC_ODE',\n",
      "       'MISCMO', 'MISCMO_', 'MISCGFTC', 'MISC_FTC', 'MISCEXPX', 'MISC_XPX',\n",
      "       'MISCDE1', 'MISCDE1_', 'MISCDE2', 'MISCDE2_', 'MISCDE3', 'MISCDE3_',\n",
      "       'MISCDE4', 'MISCDE4_', 'MISCDE5', 'MISCDE5_', 'MISCDE6', 'MISCDE6_',\n",
      "       'MISCDE7', 'MISCDE7_', 'MISCDE8', 'MISCDE8_', 'MISCDE9', 'MISPURCH',\n",
      "       'MISCDE9_'],\n",
      "      dtype='object')\n",
      "Index(['QYEAR', 'NEWID', 'SEQNO', 'ALCNO', 'REC_ORIG', 'VEHICYB', 'VEHICYB_',\n",
      "       'VEHICYR', 'VEHICYR_', 'VEHBSNZ', 'VEHBSNZ_', 'VEHNEWU', 'VEHNEWU_',\n",
      "       'VEHGFTC', 'VEHGFTC_', 'VEHPURMO', 'VEHP_RMO', 'VEHPURYR', 'VEHP_RYR',\n",
      "       'VFINSTAT', 'VFIN_TAT', 'TRADEX', 'TRADEX_', 'NETPURX', 'NETPURX_',\n",
      "       'DNPAYMTX', 'DNPA_MTX', 'PRINCIPX', 'PRIN_IPX', 'VEHQPMT', 'VEHQPMT_',\n",
      "       'PMT1MO', 'PMT1MO_', 'PMT1YR', 'PMT1YR_', 'PAYMENTX', 'PAYM_NTX',\n",
      "       'QINTRSTZ', 'QINT_STZ', 'QLOANM1Q', 'QLOA_M1Q', 'QBALNM1X', 'QBAL_M1X',\n",
      "       'QVINTM1X', 'QVIN_M1X', 'QVPRIM1X', 'QVPR_M1X', 'QLOANM2Q', 'QLOA_M2Q',\n",
      "       'QBALNM2X', 'QBAL_M2X', 'QVINTM2X', 'QVIN_M2X', 'QVPRIM2X', 'QVPR_M2X',\n",
      "       'QLOANM3Q', 'QLOA_M3Q', 'QBALNM3X', 'QBAL_M3X', 'QVINTM3X', 'QVIN_M3X',\n",
      "       'QVPRIM3X', 'QVPR_M3X', 'QTRADEX', 'QTRADEX_', 'QREIMBRZ', 'QREI_BRZ',\n",
      "       'QADITR1X', 'QADI_R1X', 'QADITR2X', 'QADI_R2X', 'QADITR3X', 'QADI_R3X',\n",
      "       'QDNPYMTX', 'QDNP_MTX', 'VEHEQTLN', 'VEHE_TLN', 'VEHICIB', 'VEHICIB_',\n",
      "       'MAKE', 'MAKE_', 'OVBPURCH', 'VFINANCE', 'FUELTYPE', 'FUEL_YPE',\n",
      "       'VPURINDV', 'VPUR_NDV', 'VINTRATE', 'VINT_ATE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(specific_expense_df1.columns)\n",
    "print(specific_expense_df2.columns)\n",
    "print(specific_expense_df3.columns)\n",
    "print(specific_expense_df4.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the specific purchase Data Frames for the relevant columns.\n",
    "- 'SEQNO' is the identifier variable for the purchases and can be used to merge with expense data frames. \n",
    "- 'APBPURCH' tells us if this item was purchased online or in-person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apb_columns_keep = ['NEWID', 'SEQNO', 'APBPURCH']\n",
    "specific_expense_df1 = specific_expense_df1[apb_columns_keep]\n",
    "specific_expense_df1_subset = specific_expense_df1.loc[specific_expense_df1['APBPURCH'] == 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'EQBPURCH' tells us if this item was bought online or in-person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqb_columns_keep = ['NEWID', 'SEQNO', 'EQBPURCH']\n",
    "specific_expense_df2 = specific_expense_df2[eqb_columns_keep]\n",
    "specific_expense_df2_subset = specific_expense_df2.loc[specific_expense_df2['EQBPURCH'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'MISPURCH' tells us if this item was bought online or in-person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_columns_keep = ['NEWID', 'SEQNO', 'MISPURCH']\n",
    "specific_expense_df3 = specific_expense_df3[mis_columns_keep]\n",
    "specific_expense_df3_subset = specific_expense_df3.loc[specific_expense_df3['MISPURCH'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'OVBPURCH' tells us if this item was bought online or in-person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovb_columns_keep = ['NEWID', 'SEQNO', 'OVBPURCH']\n",
    "specific_expense_df4 = specific_expense_df4[ovb_columns_keep]\n",
    "specific_expense_df4_subset = specific_expense_df4.loc[specific_expense_df4['OVBPURCH'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error merging with          NEWID  SEQNO  APBPURCH\n",
      "30     5092244      1       1.0\n",
      "41     5092964      2       1.0\n",
      "46     5093064      1       1.0\n",
      "51     5093204      3       1.0\n",
      "74     5093904      3       1.0\n",
      "...        ...    ...       ...\n",
      "18727  5604121      1       1.0\n",
      "18739  5604603      1       1.0\n",
      "18784  5605161      2       1.0\n",
      "18792  5605381      1       1.0\n",
      "18838  5606281      1       1.0\n",
      "\n",
      "[968 rows x 3 columns]: 'NEWID'\n",
      "Error merging with         NEWID  SEQNO  EQBPURCH\n",
      "424   5235963      9       1.0\n",
      "536   5269532     10       1.0\n",
      "660   5290251      3       1.0\n",
      "837   5359081      7       1.0\n",
      "1087  5235964     11       1.0\n",
      "1104  5253023     10       1.0\n",
      "1132  5256643      3       1.0\n",
      "1291  5277993      5       1.0\n",
      "1322  5282493      3       1.0\n",
      "1383  5291672      8       1.0\n",
      "1572  5362143      6       1.0\n",
      "1646  5418991      3       1.0\n",
      "1700  5426961     10       1.0\n",
      "2005  5274184      9       1.0\n",
      "2211  5347663      6       1.0\n",
      "2272  5357163      2       1.0\n",
      "2984  5359734      5       1.0\n",
      "2995  5361434      9       1.0\n",
      "3419  5579341      5       1.0\n",
      "3523  5599881      5       1.0: 'NEWID'\n",
      "Error merging with          NEWID  SEQNO  MISPURCH\n",
      "16     5090784     37       1.0\n",
      "18     5090784     39       1.0\n",
      "53     5091624     11       1.0\n",
      "152    5093904     28       1.0\n",
      "250    5096174     25       1.0\n",
      "...        ...    ...       ...\n",
      "30901  5603041     62       1.0\n",
      "30902  5603041     63       1.0\n",
      "31047  5605481     15       1.0\n",
      "31048  5605481     16       1.0\n",
      "31052  5605541     19       1.0\n",
      "\n",
      "[653 rows x 3 columns]: 'NEWID'\n",
      "Error merging with          NEWID  SEQNO  OVBPURCH\n",
      "92     5092244     22       1.0\n",
      "231    5094674     25       1.0\n",
      "270    5095294     32       1.0\n",
      "434    5098054     27       1.0\n",
      "680    5102544     45       1.0\n",
      "...        ...    ...       ...\n",
      "34444  5600451     29       1.0\n",
      "34521  5601811     31       1.0\n",
      "34599  5603001     31       1.0\n",
      "34674  5604241      9       1.0\n",
      "34752  5605391     11       1.0\n",
      "\n",
      "[317 rows x 3 columns]: 'NEWID'\n"
     ]
    }
   ],
   "source": [
    "def merge_expense_dfs(total_expense_df, specific_expense_dfs):\n",
    "  \"\"\"\n",
    "  Merges multiple expense DataFrames with the total expense DataFrame using left outer join.\n",
    "\n",
    "  Args:\n",
    "      total_expense_df: The DataFrame containing total expenses.\n",
    "      specific_expense_dfs: A list of DataFrames containing specific expenses.\n",
    "\n",
    "  Returns:\n",
    "      The merged DataFrame containing all expense details.\n",
    "  \"\"\"\n",
    "  merged_df = total_expense_df.copy()  # Avoid modifying original DataFrame\n",
    "  for df in specific_expense_dfs:\n",
    "    # Check for missing columns before merging\n",
    "    if 'NEWID' not in df.columns:\n",
    "      print(f\"Warning: 'NEWID' column not found in {df}\")\n",
    "      continue  # Skip merging if the column is missing\n",
    "\n",
    "    try:\n",
    "      merged_df = pd.merge(merged_df, df, on=['NEWID', 'SEQNO'], how='left')\n",
    "    except KeyError as e:\n",
    "      print(f\"Error merging with {df}: {e}\")\n",
    "  return merged_df\n",
    "\n",
    "# Example usage\n",
    "merged_df = merge_expense_dfs(total_expense_df, [specific_expense_df1_subset, specific_expense_df2_subset, specific_expense_df3_subset, specific_expense_df4_subset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the merged expense dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_expense_df = pd.merge(expense_df, specific_expense_df1_subset, on=['NEWID', 'SEQNO'], how='left')\n",
    "merged_expense_df = pd.merge(merged_expense_df, specific_expense_df2_subset, on=['NEWID', 'SEQNO'], how='left')\n",
    "merged_expense_df = pd.merge(merged_expense_df, specific_expense_df3_subset, on=['NEWID', 'SEQNO'], how='left')\n",
    "merged_expense_df = pd.merge(merged_expense_df, specific_expense_df4_subset, on=['NEWID', 'SEQNO'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the expense data we aggregate to find total yearly expenses per individual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18871, 2)\n"
     ]
    }
   ],
   "source": [
    "def calculate_total_expenses(df):\n",
    "\n",
    "#use reset_index to make a hierarchical index a regular column\n",
    "  total_expense_df = df.groupby('NEWID')['COST'].sum().reset_index() \n",
    "  \n",
    "  # Rename columns in place\n",
    "  total_expense_df.columns = ['id', 'total_expense']\n",
    "  return total_expense_df\n",
    "\n",
    "# Calculate total income for each DataFrame\n",
    "total_expense_df1 = calculate_total_expenses(expense_1_df)\n",
    "total_expense_df2 = calculate_total_expenses(expense_2_df)\n",
    "total_expense_df3 = calculate_total_expenses(expense_3_df)\n",
    "total_expense_df4 = calculate_total_expenses(expense_4_df)\n",
    "\n",
    "# Concatenate dataframes to get total income per survey participant\n",
    "total_expense_df = pd.concat([total_expense_df1, total_expense_df2, total_expense_df3, total_expense_df4], axis = 0)\n",
    "\n",
    "#DF of income over a year\n",
    "print(total_expense_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
